{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.logger.set_level(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'Pendulum-v0'\n",
    "input_dim = 3\n",
    "action_list = [4/10*i-2 for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, max_ep=0, is_global=False):\n",
    "        super(A3C, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_ep = max_ep\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, action_dim)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.ep_counter = None\n",
    "        self.ep_returns = None\n",
    "        self.average_returns = None\n",
    "        \n",
    "        if is_global:\n",
    "            self.set_global()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        Q = self.fc4(x)\n",
    "        V = self.fc5(x)\n",
    "        return Q, V\n",
    "    \n",
    "    def set_global(self):\n",
    "        self.ep_counter = mp.Value('i')\n",
    "        self.ep_counter.value = 0\n",
    "        self.ep_returns = mp.Array('d', self.max_ep)\n",
    "        self.average_returns = mp.Array('d',self.max_ep)\n",
    "        \n",
    "    def log_episode(self, ep_return):\n",
    "        c = self.ep_counter.value\n",
    "        self.ep_returns[c] = ep_return\n",
    "        self.average_returns[c] = np.mean(self.ep_returns[max(0, c-99):c+1])\n",
    "        self.ep_counter.value += 1\n",
    "        return self.ep_counter.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 8\n",
    "\n",
    "#T_max = 10000\n",
    "MAX_EP = 20000\n",
    "t_max = 5\n",
    "print_freq = 500\n",
    "\n",
    "beta = 0.01   # entropy regularization\n",
    "gamma = 0.99\n",
    "alpha = 0.99   # RMSProb decay factor\n",
    "learning_rate = 1e-2\n",
    "decay_rate = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lock, globalNet, optimizer, scheduler, tmax, pid):\n",
    "    t = 0\n",
    "    done = False\n",
    "    ep_return = 0\n",
    "    log_episode_return = []\n",
    "    cur_ep = 0\n",
    "    \n",
    "    localNet = A3C(input_dim, len(action_list))\n",
    "    localNet.load_state_dict(globalNet.state_dict())\n",
    "    env = gym.make(ENV_NAME)\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while globalNet.ep_counter.value < MAX_EP:\n",
    "        t_start = t\n",
    "        buff_value = []\n",
    "        buff_q = []\n",
    "        buff_reward = []\n",
    "        buff_logp = []\n",
    "        buff_entropy = []\n",
    "        \n",
    "        while t_start-t < t_max:\n",
    "            Q, V = localNet(torch.tensor(obs.astype(np.float32)))\n",
    "            prob = F.softmax(Q, dim=0).data\n",
    "            [a] = np.random.choice(localNet.action_dim, 1, p=prob.detach().numpy())\n",
    "            log_prob = F.log_softmax(Q, dim=0)\n",
    "\n",
    "            obs, reward, done, _ = env.step([action_list[a]])\n",
    "            ep_return += reward\n",
    "            entropy = -log_prob*prob.sum()\n",
    "\n",
    "            buff_q.append(Q)\n",
    "            buff_value.append(V)\n",
    "            buff_reward.append(reward)\n",
    "            buff_logp.append(log_prob[a])\n",
    "            buff_entropy.append(entropy)\n",
    "            t += 1\n",
    "            \n",
    "            if done:\n",
    "                cur_ep = globalNet.log_episode(ep_return)\n",
    "                obs = env.reset()\n",
    "                ep_return = 0\n",
    "                break\n",
    "\n",
    "        R = V if not done else 0\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        entropy_loss = 0\n",
    "        for i in range(-1, -(t-t_start)-1, -1): #range(t-1, t_start-1, -1):\n",
    "            R = buff_reward[i] + gamma*R\n",
    "            TD = R - buff_value[i]\n",
    "            policy_loss += buff_logp[i] * TD.detach()\n",
    "            value_loss += torch.pow(TD, 2)\n",
    "            entropy_loss += buff_entropy[i].sum()\n",
    "        loss = - policy_loss + value_loss - beta*entropy_loss\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        lock.acquire()\n",
    "        try:\n",
    "            for local_param, global_param in zip(localNet.parameters(), globalNet.parameters()):\n",
    "                global_param.grad = local_param.grad\n",
    "            optimizer.step()\n",
    "        finally:\n",
    "            lock.release()\n",
    "        localNet.load_state_dict(globalNet.state_dict())\n",
    "        \n",
    "        if cur_ep%print_freq==0: #globalNet.ep_counter.value%100==0:\n",
    "            print('[%d] Process'%pid)\n",
    "            print('%d/%d episodes. (%.2f%%)'%(cur_ep, MAX_EP, cur_ep/MAX_EP*100))\n",
    "            print('Current learning rate:', optimizer.param_groups[0]['lr'])\n",
    "            #print(globalNet.ep_counter.value-1, 'episodes.')\n",
    "            print('Total loss:\\t', loss.data.numpy()[0])\n",
    "            print('Entropy\\t\\tPolicy\\t\\tValue')\n",
    "            print('%.2f\\t\\t%.2f\\t\\t%.2f'%(entropy_loss.data.numpy(), policy_loss.data.numpy()[0], \\\n",
    "                  value_loss.data.numpy()[0]))\n",
    "            print('Epside Return: [%.1f]'%globalNet.average_returns[globalNet.ep_counter.value-1])\n",
    "            print()\n",
    "        \n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] Process\n",
      "500/20000 episodes. (2.50%)\n",
      "Current learning rate: 0.009417362622231678\n",
      "Total loss:\t 15785407.0\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "29412306.00\t\t0.00\t\t16079530.00\n",
      "Epside Return: [-1296.3]\n",
      "\n",
      "[2] Process\n",
      "1000/20000 episodes. (5.00%)\n",
      "Current learning rate: 0.008824417114557703\n",
      "Total loss:\t 10107486.0\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "41966480.00\t\t0.00\t\t10527151.00\n",
      "Epside Return: [-1273.9]\n",
      "\n",
      "[7] Process\n",
      "1500/20000 episodes. (7.50%)\n",
      "Current learning rate: 0.008301963316171971\n",
      "Total loss:\t 4089838.8\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "62426184.00\t\t0.00\t\t4714100.50\n",
      "Epside Return: [-1259.9]\n",
      "\n",
      "[7] Process\n",
      "2000/20000 episodes. (10.00%)\n",
      "Current learning rate: 0.00779482856973964\n",
      "Total loss:\t 4808253.0\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "161431152.00\t\t0.00\t\t6422564.50\n",
      "Epside Return: [-1302.5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "globalNet = A3C(input_dim, len(action_list), MAX_EP, is_global=True)\n",
    "globalNet.share_memory()\n",
    "optimizer = optim.Adam(globalNet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "lock = mp.Lock()\n",
    "\n",
    "processes = []\n",
    "for p_idx in range(NUM_THREADS):\n",
    "    p = mp.Process(target=train, args=(lock, globalNet, optimizer, scheduler, t_max, p_idx))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Learning Rate Decay & Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "average_returns = np.array(globalNet.average_returns[:])\n",
    "ep_returns = np.array(globalNet.ep_returns[:])\n",
    "nonzero_indices = average_returns!=0.0\n",
    "plt.plot(ep_returns[nonzero_indices], color='skyblue')\n",
    "plt.plot(average_returns[nonzero_indices], color='blue')\n",
    "fignum = len([f for f in os.listdir() if 'Pendulum' in f and 'png' in f])\n",
    "plt.savefig('A3C_Pendulum_%d.png'%fignum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
