{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hogun/anaconda2/envs/gym/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "GITPATH = subprocess.run('git rev-parse --show-toplevel'.split(' '), \\\n",
    "        stdout=subprocess.PIPE).stdout.decode('utf-8').replace('\\n','')\n",
    "sys.path.append(GITPATH)\n",
    "import dobroEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from utils import soft_update, OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.logger.set_level(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'DobroHalfCheetah-v0'\n",
    "state_dim = 20\n",
    "action_dim = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden1 = 64\n",
    "num_hidden2 = 64\n",
    "\n",
    "critic_lr = 1e-3\n",
    "actor_lr = 1e-3\n",
    "\n",
    "NUM_EPISODES = 10000\n",
    "batch_size = 64\n",
    "max_buff_size = 10000\n",
    "warm_up = 100\n",
    "\n",
    "decay_rate = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorNet, self).__init__()\n",
    "        \n",
    "        self.actor_layer = nn.Sequential(\n",
    "            nn.Linear(state_dim, num_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden1, num_hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden2, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.actor_layer(x)\n",
    "    \n",
    "    \n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(CriticNet, self).__init__()\n",
    "        \n",
    "        self.critic_layer = nn.Sequential(\n",
    "            nn.Linear(state_dim+action_dim, num_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden1, num_hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, actoin):\n",
    "        x = torch.cat([state, action], 0)\n",
    "        return self.critic_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_minibatch(R, batch_size):\n",
    "    minibatch = np.array(random.sample(R, batch_size))\n",
    "\n",
    "    def array_to_tensor(minibatch, idx):\n",
    "        return torch.tensor(np.stack(minibatch[:, idx]).astype(np.float32))\n",
    "\n",
    "    states = array_to_tensor(minibatch, 0)\n",
    "    actions = array_to_tensor(minibatch, 1)\n",
    "    rewards = array_to_tensor(minibatch, 2)\n",
    "    next_states = array_to_tensor(minibatch, 3)\n",
    "    return states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-70849128602e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mnext_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_anet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# mu'(s_(i+1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_cnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# q'(s_(i+1), mu'(s_(i+1)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0my_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_q_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# q(s_i, a_i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/gym/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "cnet = CriticNet(state_dim, action_dim)\n",
    "anet = ActorNet(state_dim, action_dim)\n",
    "target_cnet = CriticNet(state_dim, action_dim)\n",
    "target_anet = ActorNet(state_dim, action_dim)\n",
    "target_cnet.load_state_dict(cnet.state_dict())\n",
    "target_anet.load_state_dict(anet.state_dict())\n",
    "\n",
    "critic_optim = optim.Adam(cnet.parameters(), lr=critic_lr, weight_decay=1e-5)\n",
    "actor_optim = optim.Adam(anet.parameters(), lr=actor_lr, weight_decay=1e-5)\n",
    "critic_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=critic_optim, gamma=decay_rate)\n",
    "actor_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=actor_optim, gamma=decay_rate)\n",
    "\n",
    "random_process = OrnsteinUhlenbeckProcess(theta=0.15, mu=0., sigma=0.2, size=action_dim)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "R = []\n",
    "ridx = 0\n",
    "env = gym.make(ENV_NAME)\n",
    "env.unwrapped.initialize()\n",
    "\n",
    "for ne in range(NUM_EPISODES):\n",
    "    # init random process N\n",
    "    random_process.reset_states()\n",
    "    obs = env.reset()\n",
    "    state = torch.tensor(obs.astype(np.float32))\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state = torch.tensor(obs.astype(np.float32))\n",
    "        a = anet(state).detach().numpy()\n",
    "        noise = random_process.sample()\n",
    "        action = a + noise\n",
    "\n",
    "        pre_obs = obs\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # [s_t, a_t, r_t, s_(t+1)]\n",
    "        replay_data = [pre_obs, action, reward, obs]\n",
    "        if len(R)<max_buff_size:\n",
    "            R.append(replay_data)\n",
    "            if len(R)<warm_up:\n",
    "                continue\n",
    "        else:\n",
    "            R[ridx%max_buff_size] = replay_data\n",
    "            ridx += 1\n",
    "        \n",
    "        # sample minibatch\n",
    "#         minibatch = np.array(random.sample(R, batch_size))\n",
    "#         states = torch.tensor(minibatch[:, 0])\n",
    "#         actions = torch.tensor(minibatch[:, 1])\n",
    "#         rewards = torch.tensor(minibatch[:, 2])\n",
    "#         next_states = torch.tensor(minibatch[:, 3])\n",
    "        states, actions, rewards, next_states = sample_minibatch(R, batch_size)\n",
    "\n",
    "        next_actions = target_anet(next_states) # mu'(s_(i+1))\n",
    "        next_q_values = target_cnet(next_states, next_actions) # q'(s_(i+1), mu'(s_(i+1)))\n",
    "        y_targets = rewards + gamma * next_q_values\n",
    "        q_values = cnet(states, actions) # q(s_i, a_i)\n",
    "\n",
    "        critic_loss = criterion(q_values, y_targets)\n",
    "        cnet.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optim.step()\n",
    "\n",
    "        actor_loss = - cnt(states, anet(states)).mean()\n",
    "        anet.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optim.step()\n",
    "\n",
    "        # update target_anet, target_cnet\n",
    "        soft_update(target_cnet, cnet)\n",
    "        soft_update(target_anet, anet)\n",
    "    \n",
    "    critir_scheduler.step()            \n",
    "    actor_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06521841421992897,\n",
       " 0.16697441722686354,\n",
       " 0.1211497992960219,\n",
       " -0.12053809875482761,\n",
       " -0.0016359760287719127,\n",
       " 0.18084830211000819]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06171432,  0.14566782,  0.07760204, -0.14458719, -0.01718974,\n",
       "        0.16206875], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0035041 , 0.0213066 , 0.04354776, 0.02404909, 0.01555376,\n",
       "       0.01877955])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0035041 , 0.0213066 , 0.04354776, 0.02404909, 0.01555376,\n",
       "       0.01877955], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.59999999, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06390915,  0.12897551,  0.10098654, -0.13140325, -0.00055187,\n",
       "        0.16599107])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-961ca2cfe21f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "minibatch = np.array(random.sample(R, 2)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 0.04567381,  0.13761569,  0.08789963, -0.13004533, -0.02985892,\n",
       "        0.13857435]),\n",
       "       array([ 0.05174735,  0.14683557,  0.08604294, -0.14483311, -0.0087649 ,\n",
       "        0.17018943])], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lock, globalNet, optimizer, scheduler, tmax, pid):\n",
    "    t = 0\n",
    "    done = False\n",
    "    ep_return = 0\n",
    "    log_episode_return = []\n",
    "    cur_ep = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    localNet = A3C_v3(input_dim, action_dim)\n",
    "    localNet.load_state_dict(globalNet.state_dict())\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env.unwrapped.initialize()\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while globalNet.ep_counter.value < MAX_EP:\n",
    "        t_start = t\n",
    "        buff_value = []\n",
    "        buff_reward = []\n",
    "        buff_logp = []\n",
    "        buff_entropy = []\n",
    "\n",
    "        while t_start-t < t_max:\n",
    "            mu, sigma, V = localNet(torch.tensor(obs.astype(np.float32)))\n",
    "            Softplus=nn.Softplus()     \n",
    "            sigma = Softplus(sigma + 1e-5) # constrain to sensible values\n",
    "            normal_dist = torch.normal(mu, sigma)\n",
    "            \n",
    "            sigma = Softplus(sigma + 1e-5) # constrain to sensible values\n",
    "            action_dist = torch.normal(mu, sigma)\n",
    "            action = action_dist.detach().numpy()\n",
    "            action = action.clip(env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            entropy = -0.5 * (torch.log(2. * np.pi * sigma) + 1.)\n",
    "            \n",
    "            # log prob: gaussian negative log-likelihood\n",
    "            log_prob = torch.log(1/torch.sqrt(2*np.pi*sigma**2)) - (action_dist-mu)**2/(2*sigma**2)\n",
    "\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            step_count += 1\n",
    "            ep_return += reward\n",
    "\n",
    "            buff_value.append(V)\n",
    "            buff_reward.append(reward)\n",
    "            buff_logp.append(log_prob.sum())\n",
    "            buff_entropy.append(entropy)\n",
    "            t += 1\n",
    "            \n",
    "            if done:\n",
    "                cur_ep = globalNet.log_episode(ep_return)\n",
    "                obs = env.reset()\n",
    "                if step_count==env._max_episode_steps:\n",
    "                    done = False\n",
    "                step_count = 0\n",
    "                ep_return = 0\n",
    "                break\n",
    "\n",
    "        R = V if not done else 0\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        entropy_loss = 0\n",
    "        for i in range(-1, -(t-t_start)-1, -1):\n",
    "            R = buff_reward[i] + gamma*R\n",
    "            TD = R - buff_value[i]\n",
    "            policy_loss += buff_logp[i] * TD.detach()\n",
    "            value_loss += torch.pow(TD, 2)\n",
    "            entropy_loss += buff_entropy[i].sum()\n",
    "        loss = - policy_loss + value_loss - beta*entropy_loss\n",
    "        \n",
    "        lock.acquire()\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for local_param, global_param in zip(localNet.parameters(), globalNet.parameters()):\n",
    "                global_param.grad = local_param.grad\n",
    "            optimizer.step()\n",
    "        finally:\n",
    "            lock.release()\n",
    "        localNet.load_state_dict(globalNet.state_dict())\n",
    "        \n",
    "        if cur_ep%print_freq==0:\n",
    "            print('[%d] Process'%pid)\n",
    "            print('%d/%d episodes. (%.2f%%)'%(cur_ep, MAX_EP, cur_ep/MAX_EP*100))\n",
    "            #print('Current learning rate:', optimizer.param_groups[0]['lr'])\n",
    "            print('Total loss:\\t', loss.data.numpy()[0])\n",
    "            print('Entropy\\t\\tPolicy\\t\\tValue')\n",
    "            print('%.2f\\t\\t%.2f\\t\\t%.2f'%(entropy_loss.data.numpy(), policy_loss.data.numpy()[0], \\\n",
    "                  value_loss.data.numpy()[0]))\n",
    "            print('Epside Return: [%.1f]'%globalNet.average_returns[globalNet.ep_counter.value-1])\n",
    "            print()\n",
    "            \n",
    "            global log_df, fig_num\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            average_returns = np.array(globalNet.average_returns[:])\n",
    "            ep_returns = np.array(globalNet.ep_returns[:])\n",
    "            nonzero_indices = average_returns!=0.0\n",
    "            plt.plot(ep_returns[nonzero_indices], color='lightgreen')\n",
    "            plt.plot(average_returns[nonzero_indices], color='green')\n",
    "            plt.savefig('A3C_v3_HalfCheetah_%d.png'%fignum)\n",
    "            \n",
    "            raw_data = [cur_ep/MAX_EP*100, cur_ep, loss.data.numpy()[0], globalNet.average_returns[globalNet.ep_counter.value-1], optimizer.param_groups[0]['lr']]\n",
    "            log_df = log_df.append(pd.Series(raw_data, index = log_df.columns), ignore_index=True)\n",
    "        \n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] Process\n",
      "500/200000 episodes. (0.25%)\n",
      "Current learning rate: 0.000938913877703549\n",
      "Total loss:\t 4622.2124\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-1318.95\t\t-1945.83\t\t2675.06\n",
      "Epside Return: [43.6]\n",
      "\n",
      "[6] Process\n",
      "1000/200000 episodes. (0.50%)\n",
      "Current learning rate: 0.0008842092457380008\n",
      "Total loss:\t 2586.9006\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-284.44\t\t2039.51\t\t4626.13\n",
      "Epside Return: [39.8]\n",
      "\n",
      "[0] Process\n",
      "1500/200000 episodes. (0.75%)\n",
      "Current learning rate: 0.000831027358976173\n",
      "Total loss:\t 10228.367\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-832.30\t\t-4088.22\t\t6139.31\n",
      "Epside Return: [46.4]\n",
      "\n",
      "[3] Process\n",
      "2000/200000 episodes. (1.00%)\n",
      "Current learning rate: 0.0007779246707428731\n",
      "Total loss:\t 1610.7308\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-821.11\t\t649.62\t\t2259.53\n",
      "Epside Return: [58.8]\n",
      "\n",
      "[1] Process\n",
      "2500/200000 episodes. (1.25%)\n",
      "Current learning rate: 0.0007311354045730212\n",
      "Total loss:\t 460.51385\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-811.46\t\t1996.93\t\t2456.64\n",
      "Epside Return: [64.5]\n",
      "\n",
      "[3] Process\n",
      "3000/200000 episodes. (1.50%)\n",
      "Current learning rate: 0.0006864731778340079\n",
      "Total loss:\t 10416.915\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-871.98\t\t-4404.69\t\t6011.35\n",
      "Epside Return: [75.5]\n",
      "\n",
      "[2] Process\n",
      "3500/200000 episodes. (1.75%)\n",
      "Current learning rate: 0.0006458302079252488\n",
      "Total loss:\t 103756.21\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-1849.06\t\t-28243.61\t\t75510.75\n",
      "Epside Return: [98.0]\n",
      "\n",
      "[6] Process\n",
      "4000/200000 episodes. (2.00%)\n",
      "Current learning rate: 0.0006082017260423352\n",
      "Total loss:\t 26752.672\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-1383.03\t\t20196.89\t\t46948.18\n",
      "Epside Return: [104.2]\n",
      "\n",
      "[0] Process\n",
      "4500/200000 episodes. (2.25%)\n",
      "Current learning rate: 0.0005716206616860867\n",
      "Total loss:\t 34067.113\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-1643.68\t\t2283.77\t\t36349.24\n",
      "Epside Return: [186.5]\n",
      "\n",
      "[5] Process\n",
      "5000/200000 episodes. (2.50%)\n",
      "Current learning rate: 0.000535094073894048\n",
      "Total loss:\t 12063.24\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-1296.41\t\t11602.94\t\t23664.88\n",
      "Epside Return: [139.6]\n",
      "\n",
      "[6] Process\n",
      "5500/200000 episodes. (2.75%)\n",
      "Current learning rate: 0.0005049268418435939\n",
      "Total loss:\t 16671.553\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-2187.63\t\t14302.93\t\t30972.30\n",
      "Epside Return: [147.1]\n",
      "\n",
      "[1] Process\n",
      "6000/200000 episodes. (3.00%)\n",
      "Current learning rate: 0.00047266199234925353\n",
      "Total loss:\t 258404.38\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-7910.37\t\t99895.75\t\t358292.22\n",
      "Epside Return: [467.9]\n",
      "\n",
      "[4] Process\n",
      "6500/200000 episodes. (3.25%)\n",
      "Current learning rate: 0.00044245886829040714\n",
      "Total loss:\t 271828.56\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-8073.22\t\t-21679.31\t\t250141.19\n",
      "Epside Return: [253.9]\n",
      "\n",
      "[0] Process\n",
      "7000/200000 episodes. (3.50%)\n",
      "Current learning rate: 0.00041876922342177456\n",
      "Total loss:\t 152238.2\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-4030.85\t\t37026.51\t\t189260.69\n",
      "Epside Return: [586.8]\n",
      "\n",
      "[3] Process\n",
      "7500/200000 episodes. (3.75%)\n",
      "Current learning rate: 0.00039122622220115044\n",
      "Total loss:\t 124637.34\n",
      "Entropy\t\tPolicy\t\tValue\n",
      "-233.10\t\t15974.07\t\t140611.17\n",
      "Epside Return: [681.5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "globalNet = A3C_v3(input_dim, action_dim, MAX_EP, is_global=True)\n",
    "globalNet.share_memory()\n",
    "\n",
    "optimizer = optim.Adam(globalNet.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "lock = mp.Lock()\n",
    "\n",
    "Softplus=nn.Softplus()\n",
    "log_df = pd.DataFrame(columns=['running', 'EP', 'Loss', 'Return', 'LR'])\n",
    "fignum = len([f for f in os.listdir() if 'v3_HalfCheetah' in f and 'png' in f])\n",
    "\n",
    "processes = []\n",
    "for p_idx in range(NUM_THREADS):\n",
    "    p = mp.Process(target=train, args=(lock, globalNet, optimizer, scheduler, t_max, p_idx))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "average_returns = np.array(globalNet.average_returns[:])\n",
    "ep_returns = np.array(globalNet.ep_returns[:])\n",
    "nonzero_indices = average_returns!=0.0\n",
    "plt.plot(ep_returns[nonzero_indices], color='lightgreen')\n",
    "plt.plot(average_returns[nonzero_indices], color='green')\n",
    "plt.show()\n",
    "#fignum = len([f for f in os.listdir() if 'v3_HalfCheetah' in f and 'png' in f])\n",
    "#plt.savefig('A3C_v3_HalfCheetah_%d.png'%fignum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
