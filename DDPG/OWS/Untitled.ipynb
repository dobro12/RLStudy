{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network\n",
    "class Policy_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,state_dim, action_dim):\n",
    "        super(Policy_net,self).__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(num_features = state_dim)\n",
    "        self.linear1 = nn.Linear(in_features = state_dim, out_features = 64)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm1d(num_features = 64)\n",
    "        self.linear2 = nn.Linear(in_features = 64, out_features = 32)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        self.linear3 = nn.Linear(in_features=32, out_features=action_dim)\n",
    "        self.activation3 = nn.Tanh()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.activation1(self.linear1(self.bn1(x)))\n",
    "        x = self.activation2(self.linear2(self.bn2(x)))\n",
    "        x = self.activation3(self.linear3(self.bn3(x))) * 2\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Value_net(nn.Module):\n",
    "    def __init__(self,state_dim, action_dim):\n",
    "        super(Value_net,self).__init__()        \n",
    "        self.network = []\n",
    "        self.bn1 = nn.BatchNorm1d(num_features = state_dim)\n",
    "        self.linear1 = nn.Linear(in_features = state_dim, out_features = 128)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm1d(num_features = 128)\n",
    "        self.linear2 = nn.Linear(in_features = 128, out_features = 32)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        \n",
    "        self.linear3 = nn.Linear(in_features=action_dim, out_features=32)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(in_features = 64, out_features=1)\n",
    "                \n",
    "    def forward(self, state, action):\n",
    "        state = self.activation1(self.linear1(self.bn1(state)))\n",
    "        state = self.bn3(self.activation2(self.linear2(self.bn2(state))))\n",
    "        action = self.activation3(self.linear3(action))\n",
    "        output = torch.cat((state,action),1)\n",
    "        output = self.linear4(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent\n",
    "class Agent():\n",
    "    def __init__(self, state_dim , action_range , seed ):\n",
    "        \n",
    "        self.action_range = action_range\n",
    "        self.action_dim = len(action_range)\n",
    "        \n",
    "        self.target_policy_net = Policy_net(state_dim,self.action_dim)\n",
    "        self.target_value_net = Value_net(state_dim,self.action_dim)\n",
    "        self.learning_policy_net = Policy_net(state_dim,self.action_dim)\n",
    "        self.learning_value_net = Value_net(state_dim,self.action_dim)\n",
    "        \n",
    "        self.target_policy_net.load_state_dict(self.learning_policy_net.state_dict())\n",
    "        self.target_value_net.load_state_dict(self.learning_value_net.state_dict())\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.actor_learning_rate = 0.0001\n",
    "        self.critic_learning_rate = 0.0001\n",
    "        self.batch_size = 64\n",
    "        self.replay_buffer = []\n",
    "        self.buffer_size = 50000\n",
    "        self.buffer_index = 0\n",
    "        self.seed = seed\n",
    "        self.target_update = 0.001\n",
    "        \n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.learning_policy_net.parameters(), lr=self.actor_learning_rate)\n",
    "        self.value_optimizer = optim.Adam(self.learning_value_net.parameters(), lr=self.critic_learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.plot_reward = []\n",
    "        \n",
    "        self.learning_policy_net.zero_grad()\n",
    "        self.learning_value_net.zero_grad()\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        \n",
    "        \n",
    "    def get_target_action(self,x):\n",
    "        return self.target_policy_net(torch.Tensor(x)).detach()\n",
    "    \n",
    "    def get_learning_action(self,x):\n",
    "        return self.learning_policy_net(torch.Tensor(x))\n",
    "    \n",
    "    def get_target_action_value(self,state,action):\n",
    "        \n",
    "        return self.target_value_net( torch.Tensor(state),torch.Tensor(action) ).detach()\n",
    "    \n",
    "    def get_learning_action_value(self,state,action):\n",
    "        return self.learning_value_net(torch.Tensor(state),torch.Tensor(action))\n",
    "    \n",
    " \n",
    "        \n",
    "        \n",
    "    def cal_target_loss(self,batch):\n",
    "        state_batch = [e[0] for e in batch]\n",
    "        action_batch = [e[1] for e in batch]\n",
    "        reward_batch = [[e[2]] for e in batch]\n",
    "        next_state_batch = [e[3] for e in batch]\n",
    "        \n",
    "        \n",
    "        #target_value = [self.get_target_value(reward_batch[i],next_state_batch[i]) for i in range(self.batch_size)]\n",
    "        #target_value = torch.stack(target_value).detach()\n",
    "        target_value = self.get_target_value(reward_batch,next_state_batch)\n",
    "        \n",
    "        learning_value = self.get_learning_action_value(state_batch, action_batch)\n",
    "        \n",
    "        loss = self.criterion(target_value, learning_value)\n",
    "        '''if(loss > 1000):\n",
    "            print(\"##########################target_value#########################\")\n",
    "            print(target_value)\n",
    "\n",
    "            print(\"##########################learning_value#########################\")\n",
    "            print(learning_value)'''\n",
    "        \n",
    "        \n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def target_net_update(self):    \n",
    "        for t_p_params , l_p_params in zip(self.target_policy_net.parameters() , self.learning_policy_net.parameters()):\n",
    "            t_p_params.data.copy_( t_p_params.data * (1-self.target_update) + self.target_update * l_p_params.data )\n",
    "        \n",
    "        for t_v_params , l_v_params in zip(self.target_value_net.parameters() , self.learning_value_net.parameters()):\n",
    "            t_v_params.data.copy_( t_v_params.data * (1-self.target_update) + self.target_update * l_v_params.data )\n",
    "                \n",
    "        \n",
    "    def get_target_value(self,reward, next_state):\n",
    "        action = self.get_target_action(next_state)\n",
    "        target_action_value = self.get_target_action_value(next_state,action)\n",
    "\n",
    "        target_value = torch.Tensor(reward) + self.gamma * target_action_value.squeeze(0)\n",
    "        \n",
    "        \n",
    "\n",
    "        return target_value        \n",
    "    def get_batch(self):\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        return batch\n",
    "\n",
    "    def set_network_train(self):\n",
    "        self.learning_policy_net.train()\n",
    "        self.learning_value_net.train()\n",
    "        self.target_policy_net.train()\n",
    "        self.target_value_net.train()        \n",
    "        \n",
    "######################################################################3    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        self.learning_policy_net.zero_grad()\n",
    "        self.learning_value_net.zero_grad()       \n",
    "        self.target_policy_net.zero_grad()\n",
    "        self.target_value_net.zero_grad()               \n",
    "        \n",
    "        self.set_network_train()\n",
    "        \n",
    "        batch = self.get_batch()\n",
    "        \n",
    "        \n",
    "        target_loss =self.cal_target_loss(batch)\n",
    "        target_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        self.learning_value_net.zero_grad()\n",
    "        \n",
    "        state_batch = [e[0] for e in batch]\n",
    "        d_action = self.get_learning_action(state_batch)\n",
    "        action_value = self.get_learning_action_value(state_batch,d_action)\n",
    "        \n",
    "        mean_action_value = -torch.mean(action_value)\n",
    "        mean_action_value.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        self.learning_policy_net.zero_grad()\n",
    "        self.learning_value_net.zero_grad()\n",
    "        self.target_policy_net.zero_grad()\n",
    "        self.target_value_net.zero_grad()               \n",
    "        \n",
    "        self.target_net_update()\n",
    "        \n",
    "        return target_loss , mean_action_value\n",
    "       \n",
    "\n",
    "    \n",
    "        \n",
    "    def get_noise_action(self,state):\n",
    "        mean = self.get_learning_action(state)\n",
    "        rand_value = np.random.rand(1)\n",
    "        noise_action = []\n",
    "        if(rand_value < 0.2):\n",
    "            for i in range(len(mean)):\n",
    "                action = (np.random.rand(1) * (self.action_range[i][1]-self.action_range[i][0]) - self.action_range[i][1]).item()\n",
    "                noise_action.append([action])\n",
    "            noise_action = np.array(noise_action)\n",
    "            return noise_action\n",
    "        else:\n",
    "            return mean.detach().numpy()\n",
    "                \n",
    "                \n",
    "        \n",
    "    def buffer_update(self, replay):\n",
    "        if(len(self.replay_buffer) < self.buffer_size):\n",
    "            self.replay_buffer.append(replay)\n",
    "            self.buffer_index += 1\n",
    "        else:\n",
    "            self.replay_buffer[self.buffer_index] = replay\n",
    "            self.buffer_index += 1\n",
    "        if(self.buffer_index == self.buffer_size):\n",
    "            self.buffer_index=0\n",
    "        \n",
    "    def set_reward_plot(self, reward_sum):\n",
    "        self.plot_reward.append(reward_sum)\n",
    "        \n",
    "    def set_network_exploration(self):\n",
    "        self.learning_policy_net.eval()\n",
    "        self.learning_value_net.eval()\n",
    "        self.target_policy_net.eval()\n",
    "        self.target_value_net.eval()\n",
    "                \n",
    "        \n",
    "    def save(self,directory):\n",
    "        torch.save({'learning_policy_net_state_dict' :  self.learning_policy_net.state_dict() ,'learning_value_net_state_dict' : self.learning_value_net.state_dict() , 'target_policy_net_state_dict' :  self.target_policy_net.state_dict() ,'target_value_net_state_dict' : self.target_value_net.state_dict() } , directory + '/pendulum_model_seed_{}.pth'.format(self.seed) )\n",
    "        \n",
    "        \n",
    "    def load(self,file_name):\n",
    "        checkpoint = torch.load(directory)\n",
    "        \n",
    "        self.learning_policy_net.load_state_dict(checkpoint['learning_policy_net_state_dict'])\n",
    "        self.learning_value_net.load_state_dict(checkpoint['learning_value_net_state_dict'])\n",
    "        self.target_policy_net.load_state_dict(checkpoint['target_policy_net_state_dict'])\n",
    "        self.target_value_net.load_state_dict(checkpoint['target_value_net_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wooseoko/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value loss : 0 \n",
      "policy loss : 0 \n",
      "reward at itr 0 : -1114.3846111880937\n",
      "step : 200\n",
      "value loss : 0 \n",
      "policy loss : 0 \n",
      "reward at itr 1 : -1345.7940663199881\n",
      "step : 400\n",
      "value loss : 0 \n",
      "policy loss : 0 \n",
      "reward at itr 2 : -1139.288008504123\n",
      "step : 600\n",
      "value loss : 0 \n",
      "policy loss : 0 \n",
      "reward at itr 3 : -1073.0910288906045\n",
      "step : 800\n",
      "value loss : 0 \n",
      "policy loss : 0 \n",
      "reward at itr 4 : -1202.4749522330512\n",
      "step : 1000\n",
      "value loss : 7921.4716796875 \n",
      "policy loss : 8.66147518157959 \n",
      "reward at itr 5 : -1653.974907981836\n",
      "step : 1200\n",
      "value loss : 6778.5400390625 \n",
      "policy loss : 62.3460807800293 \n",
      "reward at itr 6 : -1251.3148477961672\n",
      "step : 1400\n",
      "value loss : 6278.92333984375 \n",
      "policy loss : 123.60250091552734 \n",
      "reward at itr 7 : -1305.4472224922977\n",
      "step : 1600\n",
      "value loss : 6072.46533203125 \n",
      "policy loss : 191.70538330078125 \n",
      "reward at itr 8 : -861.6808054384652\n",
      "step : 1800\n",
      "value loss : 5571.83447265625 \n",
      "policy loss : 269.5920715332031 \n",
      "reward at itr 9 : -1838.9716799303635\n",
      "step : 2000\n",
      "value loss : 5006.4765625 \n",
      "policy loss : 353.61651611328125 \n",
      "reward at itr 10 : -1354.0833006405792\n",
      "step : 2200\n",
      "value loss : 4763.0673828125 \n",
      "policy loss : 446.1826171875 \n",
      "reward at itr 11 : -1356.9716635446603\n",
      "step : 2400\n",
      "value loss : 4182.56591796875 \n",
      "policy loss : 546.7347412109375 \n",
      "reward at itr 12 : -1542.2381416897301\n",
      "step : 2600\n",
      "value loss : 3528.8017578125 \n",
      "policy loss : 652.3311157226562 \n",
      "reward at itr 13 : -894.0637742418351\n",
      "step : 2800\n",
      "value loss : 2881.011474609375 \n",
      "policy loss : 763.7733764648438 \n",
      "reward at itr 14 : -771.9587303824541\n",
      "step : 3000\n",
      "value loss : 2522.503662109375 \n",
      "policy loss : 880.2481079101562 \n",
      "reward at itr 15 : -1289.2475193651794\n",
      "step : 3200\n",
      "value loss : 2166.5888671875 \n",
      "policy loss : 1003.4119873046875 \n",
      "reward at itr 16 : -749.2260963901218\n",
      "step : 3400\n",
      "value loss : 1867.7127685546875 \n",
      "policy loss : 1131.284912109375 \n",
      "reward at itr 17 : -969.6629158943324\n",
      "step : 3600\n",
      "value loss : 1637.3157958984375 \n",
      "policy loss : 1265.47802734375 \n",
      "reward at itr 18 : -1853.0481410140221\n",
      "step : 3800\n",
      "value loss : 1400.4307861328125 \n",
      "policy loss : 1403.83251953125 \n",
      "reward at itr 19 : -1169.0140254115508\n",
      "step : 4000\n",
      "value loss : 1256.3970947265625 \n",
      "policy loss : 1548.583984375 \n",
      "reward at itr 20 : -1163.0718574993355\n",
      "step : 4200\n",
      "value loss : 1156.833251953125 \n",
      "policy loss : 1695.498291015625 \n",
      "reward at itr 21 : -862.802333825819\n",
      "step : 4400\n",
      "value loss : 1072.646728515625 \n",
      "policy loss : 1849.749755859375 \n",
      "reward at itr 22 : -1373.6040604274158\n",
      "step : 4600\n",
      "value loss : 937.4551391601562 \n",
      "policy loss : 2008.258056640625 \n",
      "reward at itr 23 : -1071.0519288218181\n",
      "step : 4800\n",
      "value loss : 856.1245727539062 \n",
      "policy loss : 2171.5078125 \n",
      "reward at itr 24 : -1426.9179870555643\n",
      "step : 5000\n",
      "value loss : 855.6624145507812 \n",
      "policy loss : 2342.195556640625 \n",
      "reward at itr 25 : -889.0854130253746\n",
      "step : 5200\n",
      "value loss : 747.3389282226562 \n",
      "policy loss : 2516.279541015625 \n",
      "reward at itr 26 : -1306.4837489151244\n",
      "step : 5400\n",
      "value loss : 671.0731811523438 \n",
      "policy loss : 2691.467529296875 \n",
      "reward at itr 27 : -1854.2709570416037\n",
      "step : 5600\n",
      "value loss : 622.9664306640625 \n",
      "policy loss : 2868.18701171875 \n",
      "reward at itr 28 : -742.5061825363365\n",
      "step : 5800\n",
      "value loss : 619.3723754882812 \n",
      "policy loss : 3055.09912109375 \n",
      "reward at itr 29 : -1671.9109815701213\n",
      "step : 6000\n",
      "value loss : 564.681396484375 \n",
      "policy loss : 3251.950927734375 \n",
      "reward at itr 30 : -1172.1638055308877\n",
      "step : 6200\n",
      "value loss : 529.2998046875 \n",
      "policy loss : 3442.703369140625 \n",
      "reward at itr 31 : -891.6086320129675\n",
      "step : 6400\n",
      "value loss : 512.0817260742188 \n",
      "policy loss : 3637.014892578125 \n",
      "reward at itr 32 : -635.4132112912537\n",
      "step : 6600\n",
      "value loss : 496.65325927734375 \n",
      "policy loss : 3840.690185546875 \n",
      "reward at itr 33 : -746.5554935103669\n",
      "step : 6800\n",
      "value loss : 452.2347106933594 \n",
      "policy loss : 4046.71728515625 \n",
      "reward at itr 34 : -794.4239223772258\n",
      "step : 7000\n",
      "value loss : 402.96820068359375 \n",
      "policy loss : 4247.95556640625 \n",
      "reward at itr 35 : -1425.76734791006\n",
      "step : 7200\n",
      "value loss : 431.6745300292969 \n",
      "policy loss : 4450.25634765625 \n",
      "reward at itr 36 : -1646.9568767027051\n",
      "step : 7400\n",
      "value loss : 377.3705749511719 \n",
      "policy loss : 4649.5576171875 \n",
      "reward at itr 37 : -1434.4881712124118\n",
      "step : 7600\n",
      "value loss : 406.96929931640625 \n",
      "policy loss : 4863.6845703125 \n",
      "reward at itr 38 : -1310.9436707687103\n",
      "step : 7800\n",
      "value loss : 373.7001647949219 \n",
      "policy loss : 5080.7119140625 \n",
      "reward at itr 39 : -1777.2988203157674\n",
      "step : 8000\n",
      "value loss : 411.37689208984375 \n",
      "policy loss : 5298.46435546875 \n",
      "reward at itr 40 : -1144.6566145991228\n",
      "step : 8200\n",
      "value loss : 371.2555236816406 \n",
      "policy loss : 5500.13720703125 \n",
      "reward at itr 41 : -1695.691300508308\n",
      "step : 8400\n",
      "value loss : 353.80462646484375 \n",
      "policy loss : 5697.939453125 \n",
      "reward at itr 42 : -1081.4745075850126\n",
      "step : 8600\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "action_range = []\n",
    "for min_action, max_action in zip(env.action_space.low,env.action_space.high):\n",
    "    action_range.append([min_action,max_action])\n",
    "state_dim = len(env.observation_space.sample())\n",
    "seed = 0\n",
    "step = 0\n",
    "\n",
    "agent = Agent(state_dim, action_range, seed)\n",
    "num_iteration = 5000\n",
    "\n",
    "for iteration in range(num_iteration):\n",
    "    done = False\n",
    "    reward_sum = 0\n",
    "    observation = env.reset()\n",
    "    state = observation\n",
    "    value_loss_sum = 0\n",
    "    policy_loss_sum = 0\n",
    "    for t in range(1000):\n",
    "        agent.set_network_exploration()\n",
    "\n",
    "        action = agent.get_noise_action([state])\n",
    "        action = action.squeeze(0)\n",
    "        \n",
    "        obs , reward, done, info = env.step(action)\n",
    "        #reward /= 16.2736044\n",
    "        episode = (state, action, reward, obs)\n",
    "        agent.buffer_update(episode)\n",
    "\n",
    "        step += 1\n",
    "        if(step > 1000):\n",
    "            value, policy = agent.train()\n",
    "            value_loss_sum += value\n",
    "            policy_loss_sum += policy\n",
    "        state = obs[:]\n",
    "        if(done):\n",
    "            break\n",
    "    observation = env.reset()\n",
    "    state = observation\n",
    "    for t in range(1000):\n",
    "        agent.set_network_exploration()\n",
    "        action = agent.get_learning_action([state])\n",
    "        action = action.squeeze(0)\n",
    "        obs , reward, done, info = env.step(action.detach().numpy())\n",
    "        \n",
    "        reward_sum += reward\n",
    "        state = obs[:]\n",
    "        if(done):\n",
    "            break\n",
    "    agent.set_reward_plot(reward_sum) \n",
    "    if(iteration%100 == 0):\n",
    "        agent.save('./model_pendulum')\n",
    "    print(\"value loss : {} \".format(value_loss_sum))\n",
    "    print(\"policy loss : {} \".format(policy_loss_sum))\n",
    "    print(\"reward at itr {} : {}\".format(iteration, reward_sum))\n",
    "    print(\"step : {}\".format(step))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6_spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
