{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lgm/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "###성공###\n",
    "import sys\n",
    "sys.path.append('/home/lgm/JEONGHO/RL스터디2020/')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import dobroEnv\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "# from halfCheetah import HalfCheetah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('DobroHalfCheetah-v0')\n",
    "# env.unwrapped.initialize(is_render=True) # renderin하려면.\n",
    "GAME_NAME = 'HalfCheetah-v3'\n",
    "\n",
    "env = gym.make(GAME_NAME)\n",
    "eval_env = gym.make(GAME_NAME)\n",
    "\n",
    "MAX_EPISODE =300\n",
    "REPLAY_BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "ALPHA = 0.1\n",
    "BETA = 0.1\n",
    "SOFT_UPDATA_TAU = 0.001\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.shape[0]\n",
    "Policy_LR = 3e-4\n",
    "Qnet_LR = 3e-4\n",
    "MIN_REPLAY_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4201029 , -0.4592422 , -0.5986772 , -0.3189302 ,  0.10325069,\n",
       "       -0.11548314], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size):\n",
    "        self.history = deque([], maxlen = buffer_size)\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        buffer = (state, action, reward, next_state, done)\n",
    "        self.history.append(buffer)\n",
    "        \n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = random.sample(self.history, batch_size)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "        self.gamma = GAMMA\n",
    "        self.alpha = ALPHA\n",
    "        self.betha = BETA \n",
    "        self.tau = SOFT_UPDATA_TAU\n",
    "        self.state_dim = STATE_DIM\n",
    "        self.action_dim = ACTION_DIM\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.minimum_replay_size = MIN_REPLAY_SIZE\n",
    "        self.eval_count = 0\n",
    "        \n",
    "        self.PolicyOptimizer = tf.train.AdamOptimizer(learning_rate=Policy_LR)\n",
    "        self.QnetOptimizer = tf.train.AdamOptimizer(learning_rate=Qnet_LR)\n",
    "        \n",
    "        with tf.variable_scope('SACagent'):\n",
    "            self.state_ph = tf.placeholder(tf.float32, shape=(None, self.state_dim), name='state_ph')\n",
    "            self.action_ph = tf.placeholder(tf.float32, shape=(None, self.action_dim), name='action_ph')\n",
    "            self.sample_action_ph = tf.placeholder(tf.float32, shape=(None, self.action_dim), name='sample_action_ph')\n",
    "            self.action_eps_ph = tf.placeholder(tf.float32, shape=(None,self.action_dim), name='eps_ph')\n",
    "            self.action_std_ph = tf.placeholder(tf.float32, shape=(None,self.action_dim), name='std_ph')\n",
    "            \n",
    "            self._build_policy()\n",
    "            self._build_qnet()\n",
    "            \n",
    "            self.policy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'SACagent/policy')\n",
    "\n",
    "            self.qnet1_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'SACagent/onlineq1')\n",
    "            self.qnet2_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'SACagent/onlineq2')\n",
    "            self.qnet1_target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'SACagent/targetq1')\n",
    "            self.qnet2_target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'SACagent/targetq2')\n",
    "            self.qnet_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'SACagent/onlineq')\n",
    "            \n",
    "            self.q1_target_update_ops, self.q2_target_update_ops = self._build_q_target_update_ops()\n",
    "            \n",
    "\n",
    "            self.reward_ph = tf.placeholder(tf.float32, (None,1), name='reward_ph')\n",
    "            self.done_ph = tf.cast(tf.placeholder(tf.bool, (None,1), name='done_ph'), tf.float32)\n",
    "\n",
    "            \n",
    "            #self.log_policy = - 0.5 * tf.log(1e-8+(2.*np.pi * self.action_std**2)) \\\n",
    "             #                       - 0.5 * (self.action_eps_ph)**2 \\\n",
    "               #                     - tf.expand_dims(tf.reduce_sum(tf.log(1e-8+1 - tf.square(self.action)),axis=1),1)#sampling된 액션1(from current state of buffer)\n",
    "\n",
    "            #self.log_policy_next = - 0.5 * tf.log(1e-8+(2.*np.pi * self.action_std_ph**2)) \\\n",
    "             #                               - 0.5 * (self.action_eps_ph)**2 \\\n",
    "             #                               - tf.expand_dims(tf.reduce_sum(tf.log(1e-8+1 - tf.square(self.action_ph)),axis=1),1)#sampling 된 액션2 (from next_state of buffer)\n",
    "            self.log_policy =  -0.5 * tf.log(1e-8 + (2.*np.pi * self.action_std**2)) -0.5*(self.action_eps_ph)**2 - tf.expand_dims(tf.reduce_sum(2*(np.log(2) - self.action - tf.nn.softplus(-2*self.action)), axis=1),1)\n",
    "            self.log_policy_next =  -0.5 * tf.log(1e-8 + (2.*np.pi * self.action_std_ph**2)) -0.5*(self.action_eps_ph)**2 - tf.expand_dims(tf.reduce_sum(2*(np.log(2) - self.action_ph - tf.nn.softplus(-2*self.action_ph)), axis=1),1)\n",
    "            \n",
    "            \n",
    "            self.qnet_target = self.reward_ph + self.gamma * (1 - self.done_ph) * (self.qmin_target - self.alpha * self.log_policy_next) # 전부 샘플링된 액션2 (from next_state of buffer)\n",
    "            self.qnet_target_ph = tf.placeholder(tf.float32, shape=(None, self.action_dim), name='qnet_target_ph')\n",
    "\n",
    "            # 폴리시 LOSS\n",
    "            self.policy_loss = tf.reduce_mean(self.alpha * self.log_policy - self.qmin_backprop)\n",
    "        \n",
    "            # q네트워크 LOSS\n",
    "            self.qnet1_loss = tf.reduce_mean(tf.square(self.q_value1 - self.qnet_target_ph)) # q_value1: buffer's current action, current state\n",
    "            self.qnet2_loss = tf.reduce_mean(tf.square(self.q_value2 - self.qnet_target_ph)) \n",
    "            self.qnet_loss = self.qnet1_loss + self.qnet2_loss\n",
    "\n",
    "            self.qnet_update = self.QnetOptimizer.minimize(self.qnet_loss, var_list=self.qnet_vars)\n",
    "            \n",
    "            self.policy_update = self.PolicyOptimizer.minimize(self.policy_loss, var_list=self.policy_vars)\n",
    "#         print('HHHHHHHHHHHHHHHHHHHHHHHHHHHHH')\n",
    "#         self.q_initial_sync()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def _build_qnet(self):\n",
    "        LAYER_SIZE = 128\n",
    "        init_mu = 0.\n",
    "        init_std = 0.03\n",
    "        qnet_input = tf.concat([self.state_ph, self.action_ph], axis=1)\n",
    "        with tf.variable_scope('onlineq1'):\n",
    "            qnet_hid11 = tf.layers.dense(inputs=qnet_input, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q1_h1')\n",
    "            qnet_hid11 = tf.nn.relu(qnet_hid11)\n",
    "            qnet_hid12 = tf.layers.dense(inputs=qnet_hid11, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q1_h2')\n",
    "            qnet_hid12 = tf.nn.relu(qnet_hid12)\n",
    "\n",
    "            self.q_value1 = tf.layers.dense(inputs=qnet_hid12, units=self.action_dim, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q1_h3')\n",
    "        \n",
    "        with tf.variable_scope('onlineq2'):\n",
    "            qnet_hid21 = tf.layers.dense(inputs=qnet_input, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q2_h1')\n",
    "            qnet_hid21 = tf.nn.relu(qnet_hid21)\n",
    "            qnet_hid22 = tf.layers.dense(inputs=qnet_hid21, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q2_h2')\n",
    "            qnet_hid22 = tf.nn.relu(qnet_hid22)\n",
    "\n",
    "            self.q_value2 = tf.layers.dense(inputs=qnet_hid22, units=self.action_dim, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q2_h3')\n",
    "        self.qmin = tf.minimum(self.q_value1, self.q_value2)\n",
    "        \n",
    "        qnet_target_input = tf.concat([self.state_ph, self.sample_action_ph], axis=1)\n",
    "        with tf.variable_scope('targetq1'):\n",
    "            qnet_target_hid11 = tf.layers.dense(inputs=qnet_target_input, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std),name='qt1_h1')\n",
    "            qnet_target_hid11 = tf.nn.relu(qnet_target_hid11)\n",
    "            qnet_target_hid12 = tf.layers.dense(inputs=qnet_target_hid11, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std),name='qt1_h2')\n",
    "            qnet_target_hid12 = tf.nn.relu(qnet_target_hid12)\n",
    "\n",
    "            self.q_value1_target = tf.layers.dense(inputs=qnet_target_hid12, units=self.action_dim, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='qt1_h3')\n",
    "        \n",
    "        with tf.variable_scope('targetq2'):\n",
    "            qnet_target_hid21 = tf.layers.dense(inputs=qnet_target_input, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='qt2_h1')\n",
    "            qnet_target_hid21 = tf.nn.relu(qnet_target_hid21)\n",
    "            qnet_target_hid22 = tf.layers.dense(inputs=qnet_target_hid21, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='qt2_h2')\n",
    "            qnet_target_hid22 = tf.nn.relu(qnet_target_hid22)\n",
    "\n",
    "            self.q_value2_target = tf.layers.dense(inputs=qnet_target_hid22, units=self.action_dim, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='qt2_h3')\n",
    "        self.qmin_target = tf.minimum(self.q_value1_target, self.q_value2_target)\n",
    "        \n",
    "        #### q에 들어가는 action이 self.action이어서 grad가 back prop되는 q들\n",
    "        qnetg_input = tf.concat([self.state_ph, self.action], axis=1)\n",
    "        with tf.variable_scope('onlineq1', reuse=True):\n",
    "            qnetg_hid11 = tf.layers.dense(inputs=qnetg_input, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q1_h1')\n",
    "            qnetg_hid11 = tf.nn.relu(qnetg_hid11)\n",
    "            qnetg_hid12 = tf.layers.dense(inputs=qnetg_hid11, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q1_h2')\n",
    "            qnetg_hid12 = tf.nn.relu(qnetg_hid12)\n",
    "\n",
    "            self.q_value1_copy = tf.layers.dense(inputs=qnetg_hid12, units=self.action_dim, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q1_h3')\n",
    "        with tf.variable_scope('onlineq2', reuse=True):\n",
    "            qnetg_hid21 = tf.layers.dense(inputs=qnetg_input, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q2_h1')\n",
    "            qnetg_hid21 = tf.nn.relu(qnetg_hid21)\n",
    "            qnetg_hid22 = tf.layers.dense(inputs=qnetg_hid21, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q2_h2')\n",
    "            qnetg_hid22 = tf.nn.relu(qnetg_hid22)\n",
    "\n",
    "            self.q_value2_copy = tf.layers.dense(inputs=qnetg_hid22, units=self.action_dim, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std),name='q2_h3')\n",
    "        self.qmin_backprop = tf.minimum(self.q_value1_copy, self.q_value2_copy)\n",
    "        \n",
    "    def _build_policy(self):\n",
    "        LAYER_SIZE = 128\n",
    "        init_mu = 0.\n",
    "        init_std = 0.03\n",
    "        with tf.variable_scope('policy'):\n",
    "            policy_hid1 = tf.layers.dense(inputs=self.state_ph, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std))\n",
    "            policy_hid1 = tf.nn.relu(policy_hid1)\n",
    "            \n",
    "            policy_hid2 = tf.layers.dense(inputs=policy_hid1, units=LAYER_SIZE, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std))\n",
    "            policy_hid2 = tf.nn.relu(policy_hid2)\n",
    "            \n",
    "            self.action_mu = tf.layers.dense(inputs=policy_hid2, units=self.action_dim, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std))\n",
    "            self.action_std = tf.layers.dense(inputs=policy_hid2, units=self.action_dim, kernel_initializer=tf.compat.v1.initializers.random_normal(init_mu,init_std), bias_initializer= tf.compat.v1.initializers.random_normal(init_mu,init_std))\n",
    "#             self.action_std = tf.nn.softplus(self.action_std)\n",
    "            self.action_std = tf.clip_by_value(self.action_std, -20,2)\n",
    "            self.action_std = tf.exp(self.action_std)\n",
    "            \n",
    "\n",
    "\n",
    "            self.unbounded_action = self.action_mu + self.action_std * self.action_eps_ph\n",
    "            self.action = tf.nn.tanh(self.unbounded_action)\n",
    "\n",
    "\n",
    "\n",
    "    def q_initial_sync(self):\n",
    "        ops1 = []\n",
    "        for target_var, online_var in zip(self.qnet1_target_vars, self.qnet1_vars):\n",
    "            ops1.append(target_var.assign(online_var))\n",
    "        ops2 = []\n",
    "        for target_var, online_var in zip(self.qnet2_target_vars, self.qnet2_vars):\n",
    "            ops2.append(target_var.assign(online_var))\n",
    "        for op in ops1:\n",
    "            sess.run(op)\n",
    "        for op in ops2:\n",
    "            sess.run(op)\n",
    "        print('#########################\\nQ net initially sync done\\n#########################')\n",
    "        \n",
    "    def _build_q_target_update_ops(self):\n",
    "        ops1 = []\n",
    "        for target_var, online_var in zip(self.qnet1_target_vars, self.qnet1_vars):\n",
    "            ops1.append(target_var.assign(self.tau * online_var + (1-self.tau) * target_var))\n",
    "        ops2 = []\n",
    "        \n",
    "        for target_var, online_var in zip(self.qnet2_target_vars, self.qnet2_vars):\n",
    "            ops2.append(target_var.assign(self.tau * online_var + (1-self.tau) * target_var))\n",
    "        return ops1, ops2\n",
    "\n",
    "    \n",
    "    def q_target_update(self):\n",
    "        for op in self.q1_target_update_ops:\n",
    "            sess.run(op)\n",
    "        for op in self.q2_target_update_ops:\n",
    "            sess.run(op)\n",
    "    \n",
    "        \n",
    "    def sample_action(self, state, Train=False):\n",
    "        if Train:\n",
    "            eps = self.sample_action_eps(Train=True)\n",
    "            \n",
    "            action = sess.run(self.action, feed_dict={self.state_ph:state, self.action_eps_ph:eps})\n",
    "\n",
    "            return action, eps\n",
    "        else:\n",
    "            if len(self.replay.history) < self.minimum_replay_size:\n",
    "                action = [env.action_space.sample()]\n",
    "\n",
    "                ua = action\n",
    "            else:\n",
    "                eps = self.sample_action_eps()\n",
    "                action, ua = sess.run([self.action,self.unbounded_action], feed_dict={self.state_ph:[state], self.action_eps_ph:[eps]})\n",
    "\n",
    "            return action, ua\n",
    "    \n",
    "    def sample_action_eval(self, state):\n",
    "#         eps = self.sample_action_eps()\n",
    "        action = sess.run(self.action_mu, feed_dict={self.state_ph:[state]})\n",
    "    \n",
    "        return np.tanh(action)\n",
    "\n",
    "        \n",
    "    def sample_action_eps(self, Train=False):\n",
    "        if Train:\n",
    "            action_eps = np.random.normal(size=(self.batch_size, self.action_dim))\n",
    "#             print('예시', action_eps)\n",
    "        else:\n",
    "            action_eps = np.random.normal(size=self.action_dim) # shape 아마 틀릴듯\n",
    "        return action_eps\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        batch = self.replay.sample_batch(self.batch_size)\n",
    "        return batch\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    def train_policy(self, states,actions,rewards,next_states,dones):\n",
    "\n",
    "        eps = self.sample_action_eps(Train=True)\n",
    "\n",
    "        feed_dict = {self.state_ph:states, self.action_eps_ph:eps}\n",
    "        _, ploss = sess.run([self.policy_update, self.policy_loss], feed_dict=feed_dict)\n",
    "\n",
    "        return ploss\n",
    "    \n",
    "    def train_qnet(self, states,actions,rewards,next_states,dones):\n",
    "\n",
    "        eps = self.sample_action_eps(Train=True)\n",
    "\n",
    "        next_actions, stds = sess.run([self.action, self.action_std], feed_dict={self.state_ph:next_states, self.action_eps_ph:eps})\n",
    "        targets = sess.run(self.qnet_target, feed_dict={self.reward_ph:rewards, self.done_ph: dones, self.state_ph:next_states, self.action_eps_ph:eps, self.sample_action_ph:next_actions, self.action_ph:next_actions, self.action_std_ph:stds})\n",
    "\n",
    "        _, qloss = sess.run([self.qnet_update, self.qnet_loss], feed_dict={self.state_ph:states, self.action_ph:actions, self.qnet_target_ph:targets})\n",
    "        self.q_target_update()\n",
    "\n",
    "        \n",
    "        return qloss\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay.history) < self.minimum_replay_size:\n",
    "            return 1,1\n",
    "        batches = self.sample_batch()\n",
    "        states, actions, rewards, next_states, dones = [],[],[],[],[]\n",
    "        for batch in batches:\n",
    "            states.append(batch[0])\n",
    "            actions.append(batch[1])\n",
    "            rewards.append(batch[2])\n",
    "            next_states.append(batch[3])\n",
    "\n",
    "            dones.append(batch[4])\n",
    "\n",
    "        qloss = self.train_qnet(states,actions,rewards,next_states,dones)\n",
    "        ploss = self.train_policy(states,actions,rewards,next_states,dones)\n",
    "#         ploss = 1\n",
    "        \n",
    "        return qloss, ploss\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.eval_count += 1\n",
    "        eval_s = eval_env.reset()\n",
    "        eval_score = 0\n",
    "        eval_step = 0\n",
    "        while True:\n",
    "            eval_step += 1\n",
    "            time.sleep(0.002)\n",
    "            eval_a = self.sample_action_eval(eval_s)\n",
    "#             eval_a = self.sample_action(eval_s)\n",
    "            eval_ns, eval_r, eval_d, info = eval_env.step(eval_a[0])\n",
    "            eval_score += eval_r\n",
    "            eval_s = eval_ns\n",
    "            if eval_d:\n",
    "                print(self.eval_count, '-th evaluation done. score:' ,eval_score, 'step:', eval_step)\n",
    "                break\n",
    "        \n",
    "        return eval_score\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f981a9048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f981a9048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f981a9048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f981a9048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fc02cceb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98280be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98280be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98280be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98280be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98280be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98280be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98280be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98280be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fd9795b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fd9795b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fd9795b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8fd9795b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbf28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980bbda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98120ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98120ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98120ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98120ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98120ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98120ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98120ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f98120ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f980459b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f97f79358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f97f79358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f97f79358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f97f79358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8f9818fbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "#########################\n",
      "Q net initially sync done\n",
      "#########################\n",
      "step 1000 score -183.04500398545528 qloss 1 ploss 1\n",
      "ac [array([-0.4038952 , -0.35874572, -0.05723584,  0.69171536,  0.91077   ,\n",
      "       -0.17818804], dtype=float32)] ua [array([-0.4038952 , -0.35874572, -0.05723584,  0.69171536,  0.91077   ,\n",
      "       -0.17818804], dtype=float32)]\n",
      "1 -th evaluation done. score: -0.6062568139404396 step: 1000\n",
      "step 1000 score -369.0100939215864 qloss 1 ploss 1\n",
      "ac [array([ 0.5775232 , -0.31822348, -0.8335357 ,  0.1547291 , -0.44263032,\n",
      "       -0.3177023 ], dtype=float32)] ua [array([ 0.5775232 , -0.31822348, -0.8335357 ,  0.1547291 , -0.44263032,\n",
      "       -0.3177023 ], dtype=float32)]\n",
      "2 -th evaluation done. score: 0.1088181127204615 step: 1000\n",
      "step 1000 score -288.11292053776674 qloss 1.2734358 ploss 0.09151266\n",
      "ac [array([-0.42523354, -0.12355768,  0.23213592, -0.33060104,  0.37353754,\n",
      "       -0.7122319 ], dtype=float32)] ua [array([-0.42523354, -0.12355768,  0.23213592, -0.33060104,  0.37353754,\n",
      "       -0.7122319 ], dtype=float32)]\n",
      "3 -th evaluation done. score: -2.043804847018835 step: 1000\n",
      "step 1000 score -314.30275892075906 qloss 0.06829111 ploss 0.13562953\n",
      "ac [[ 1.         -0.9253571   0.49059483  1.          0.54550487 -0.25944912]] ua [[32.23748    -1.6250756   0.5368434  35.414795    0.61195934 -0.2655177 ]]\n",
      "4 -th evaluation done. score: -248.85929010129266 step: 1000\n",
      "step 1000 score -256.9248448927205 qloss 0.07220641 ploss 0.16828698\n",
      "ac [[ 1.          0.08103626  0.16533858  1.         -0.12099102  0.17859302]] ua [[26.15422     0.08121435  0.16687042 30.827444   -0.12158665  0.180529  ]]\n",
      "5 -th evaluation done. score: -199.02762844398168 step: 1000\n",
      "step 1000 score -94.13559988067158 qloss 0.13802952 ploss 0.24667448\n",
      "ac [[ 0.9999971   0.13480574 -0.8806984  -0.9999971  -0.38148472 -0.12338941]] ua [[ 6.704376    0.13563135 -1.3788717  -6.7082243  -0.40179613 -0.1240214 ]]\n",
      "6 -th evaluation done. score: -152.44938528798204 step: 1000\n",
      "step 1000 score -192.0886435694159 qloss 0.20844351 ploss 0.25604692\n",
      "ac [[-0.6648139  -0.290382    0.04224428 -0.9999992   0.22789173 -0.15002486]] ua [[-0.80139124 -0.2989834   0.04226944 -7.3667536   0.23196457 -0.15116589]]\n",
      "7 -th evaluation done. score: -169.2336578060522 step: 1000\n",
      "step 1000 score -106.47246732730625 qloss 0.20266151 ploss 0.54627305\n",
      "ac [[ 0.0654097  -0.7022222  -0.14127943  0.15738001  0.05543996 -0.07959216]] ua [[ 0.06550322 -0.8716711  -0.14223084  0.15869904  0.05549687 -0.07976088]]\n",
      "8 -th evaluation done. score: -83.46858567835683 step: 1000\n",
      "step 1000 score -82.73656457577546 qloss 0.24294093 ploss 0.5585004\n",
      "ac [[ 0.51413286  0.03365865 -0.60095537 -0.10596997 -0.26545578 -0.53942305]] ua [[ 0.5683315   0.03367137 -0.69464135 -0.10636935 -0.27196872 -0.6033416 ]]\n",
      "9 -th evaluation done. score: -5.040230169826699 step: 1000\n",
      "step 1000 score 40.83141616155916 qloss 0.1809876 ploss 0.5492143\n",
      "ac [[ 0.64436454 -0.02854654 -0.08346841 -0.5239533  -0.32364422 -0.1894397 ]] ua [[ 0.7656015  -0.02855431 -0.08366308 -0.5817735  -0.3357124  -0.19175595]]\n",
      "10 -th evaluation done. score: -33.267490492767614 step: 1000\n",
      "step 1000 score 4.433463776244106 qloss 0.31416708 ploss 0.047893684\n",
      "ac [[-0.05292157 -0.14263704 -0.54263526 -0.38845605  0.3720723  -0.13964531]] ua [[-0.05297107 -0.14361638 -0.60788316 -0.40998048  0.39082626 -0.14056383]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 -th evaluation done. score: -40.445005383518186 step: 1000\n",
      "step 1000 score -121.78649397055128 qloss 0.21320109 ploss 0.17721023\n",
      "ac [[ 0.6881304  -0.18187153 -0.36293006  0.6277543   0.04985341 -0.16280343]] ua [[ 0.8443958  -0.18391758 -0.38025635  0.73770106  0.04989478 -0.16426513]]\n",
      "12 -th evaluation done. score: -123.34292717432282 step: 1000\n",
      "step 1000 score -133.34853786964254 qloss 0.32856548 ploss 0.68258685\n",
      "ac [[ 0.7139821   0.01628148 -0.19555646  0.7462399  -0.07583302 -0.4101067 ]] ua [[ 0.89526004  0.01628292 -0.19810812  0.96441525 -0.07597889 -0.43573952]]\n",
      "13 -th evaluation done. score: -145.2829193777933 step: 1000\n",
      "step 1000 score -112.59357038249605 qloss 0.3394901 ploss 0.45566833\n",
      "ac [[-0.23752752 -0.5076014  -0.5235186   0.01721634 -0.5888514   0.1265001 ]] ua [[-0.24215217 -0.55949324 -0.5811746   0.01721805 -0.67590594  0.12718143]]\n",
      "14 -th evaluation done. score: 68.64724339790993 step: 1000\n",
      "step 1000 score 14.652159721812632 qloss 0.33877328 ploss 0.49754646\n",
      "ac [[ 0.6470266   0.68351525 -0.48670137 -0.5371021   0.6408506  -0.23896512]] ua [[ 0.7701671   0.83568203 -0.5317286  -0.6000738   0.75961584 -0.24367628]]\n",
      "15 -th evaluation done. score: 70.17440541567618 step: 1000\n",
      "step 1000 score 43.3671862514345 qloss 0.39241537 ploss 0.4135845\n",
      "ac [[-0.01140208 -0.4640384  -0.35442746 -0.56234944 -0.35561326 -0.16337502]] ua [[-0.01140258 -0.5024457  -0.3704983  -0.63626266 -0.37185508 -0.16485232]]\n",
      "16 -th evaluation done. score: 41.22103245218031 step: 1000\n",
      "step 1000 score 136.96638391744955 qloss 0.60590357 ploss 0.25199917\n",
      "ac [[-0.7926423  -0.8657074  -0.66463155  0.9998835  -0.19906512 -0.48777506]] ua [[-1.0785003  -1.3156873  -0.8010646   4.875031   -0.20175892 -0.5331366 ]]\n",
      "17 -th evaluation done. score: 161.14633224570963 step: 1000\n",
      "step 1000 score 114.59795831113922 qloss 0.4737069 ploss 0.51523\n",
      "ac [[-0.24270277 -0.90704614 -0.32740238  0.9807171  -0.4722301   0.00223991]] ua [[-2.4764410e-01 -1.5106034e+00 -3.3991599e-01  2.3159974e+00\n",
      "  -5.1293659e-01  2.2399127e-03]]\n",
      "18 -th evaluation done. score: 194.8569476568963 step: 1000\n",
      "step 1000 score 148.09346730212073 qloss 0.54178685 ploss 0.28146926\n",
      "ac [[-0.665729    0.5319005   0.37130448  0.2793951   0.30180615  0.36312306]] ua [[-0.80303305  0.59279186  0.38993537  0.28702587  0.31150562  0.38047862]]\n",
      "19 -th evaluation done. score: 195.09783712246107 step: 1000\n",
      "step 1000 score 262.40142039638357 qloss 0.5631687 ploss 0.30739748\n",
      "ac [[-0.07550965 -0.59969133 -0.9325907   0.70884365 -0.41103688 -0.12244934]] ua [[-0.07565366 -0.692665   -1.677917    0.8848558  -0.43685827 -0.1230669 ]]\n",
      "20 -th evaluation done. score: 352.21553010283617 step: 1000\n",
      "step 1000 score 204.63342093428585 qloss 0.51029325 ploss 0.012925918\n",
      "ac [[ 0.27650452 -0.8683277  -0.9460249   0.9852507  -0.29858628  0.6702149 ]] ua [[ 0.28389323 -1.3262415  -1.79251     2.4511504  -0.3079668   0.811133  ]]\n",
      "21 -th evaluation done. score: 346.1750180644005 step: 1000\n",
      "step 1000 score 336.0699242249726 qloss 0.5058491 ploss 0.10914815\n",
      "ac [[ 0.764983    0.9873098  -0.272907   -0.9919715   0.8685153   0.47621018]] ua [[ 1.0081192   2.5268548  -0.28000206 -2.7569327   1.3270046   0.5180714 ]]\n",
      "22 -th evaluation done. score: 422.55533236114394 step: 1000\n",
      "step 1000 score 359.07687517795534 qloss 0.45384678 ploss 0.15556447\n",
      "ac [[-0.7795078  -0.9975298   0.7057223   0.9915469  -0.33666667  0.75154525]] ua [[-1.044115   -3.3476684   0.8786098   2.7310658  -0.35032833  0.97649646]]\n",
      "23 -th evaluation done. score: 512.2707795394535 step: 1000\n",
      "step 1000 score 396.13059476660595 qloss 0.53190184 ploss 0.4781643\n",
      "ac [[ 0.38399836 -0.9909469   0.50823355  0.9992653  -0.35430261  0.40323085]] ua [[ 0.40474117 -2.6966271   0.56034523  3.9545004  -0.3703555   0.42750114]]\n",
      "24 -th evaluation done. score: 516.0839577044902 step: 1000\n",
      "step 1000 score 443.93566787493654 qloss 0.52264243 ploss -0.08320927\n",
      "ac [[ 0.60138535  0.8522247   0.18402605 -0.9949679   0.04032125  0.39267743]] ua [[ 0.69531465  1.264225    0.1861467  -2.9912834   0.04034312  0.41496164]]\n",
      "25 -th evaluation done. score: 650.0455527056347 step: 1000\n",
      "step 1000 score 565.7914884867854 qloss 0.8215718 ploss -0.10720869\n",
      "ac [[ 0.6556615   0.99999875  0.197077   -0.99525356 -0.1833683  -0.02301406]] ua [[ 0.78516537  7.116716    0.19968963 -3.0205758  -0.18546599 -0.02301812]]\n",
      "26 -th evaluation done. score: 569.6697461923137 step: 1000\n",
      "step 1000 score 495.5711596948409 qloss 0.6851871 ploss -0.43462786\n",
      "ac [[-0.89214987 -0.99974775 -0.29444122  1.          0.925321    0.99999386]] ua [[-1.4323628 -4.488922  -0.3034222 11.524552   1.6248244  6.3363876]]\n",
      "27 -th evaluation done. score: 572.3723175170504 step: 1000\n",
      "step 1000 score 526.4635159344432 qloss 0.86340106 ploss 0.2658778\n",
      "ac [[ 0.07405753 -0.97767586 -0.22789136  0.9997655  -0.6208789   0.5512662 ]] ua [[ 0.07419337 -2.242005   -0.2319642   4.5255756  -0.72643405  0.6201984 ]]\n",
      "28 -th evaluation done. score: 727.0507019304656 step: 1000\n",
      "step 1000 score 152.15550745504962 qloss 0.5861233 ploss -1.6422983\n",
      "ac [[ 0.04712441 -0.9998739   0.23835233  0.999984   -0.32915682  0.40032014]] ua [[ 0.04715934 -4.836254    0.24302648  5.866295   -0.34188235  0.42403013]]\n",
      "29 -th evaluation done. score: 467.5477917542613 step: 1000\n",
      "step 1000 score 378.67110100554186 qloss 0.6024748 ploss -0.8604339\n",
      "ac [[ 0.9927335   0.9999708   0.46652007 -0.99811214  0.28284496  0.97872794]] ua [[ 2.8069873   5.565488    0.5056131  -3.4822915   0.29077178  2.2664077 ]]\n",
      "30 -th evaluation done. score: -64.70575071370712 step: 1000\n",
      "step 1000 score 129.8990074071213 qloss 0.7903029 ploss -0.8951953\n",
      "ac [[ 0.43448123 -0.01133782 -0.15693104 -0.65432245 -0.2932473  -0.26070538]] ua [[ 0.46540755 -0.01133831 -0.15823871 -0.7828201  -0.30211544 -0.2668651 ]]\n",
      "31 -th evaluation done. score: 10.77929465384838 step: 1000\n",
      "step 1000 score 111.9060212451572 qloss 0.7813068 ploss -1.0574511\n",
      "ac [[-0.14198281  0.23433267  0.27043504  0.4609522   0.21369672 -0.34946933]] ua [[-0.14294863  0.23876901  0.27733314  0.49851975  0.21704179 -0.36483914]]\n",
      "32 -th evaluation done. score: 633.7405417798874 step: 1000\n",
      "step 1000 score 554.6061098443183 qloss 0.90541023 ploss -1.9018211\n",
      "ac [[ 0.9690316   0.980475   -0.22683366 -0.99999803  0.29905885  0.57096505]] ua [[ 2.0761645   2.309699   -0.23084883 -6.9341526   0.30848572  0.6489535 ]]\n",
      "33 -th evaluation done. score: 38.12332499143782 step: 1000\n",
      "step 1000 score 383.60590355963774 qloss 0.9031344 ploss -2.162959\n",
      "ac [[ 0.37576136 -0.21208312  0.00668588 -0.8039485   0.016848    0.03893012]] ua [[ 0.3951149  -0.21535158  0.00668598 -1.109678    0.01684959  0.03894981]]\n",
      "34 -th evaluation done. score: 9.135345113176477 step: 1000\n",
      "step 1000 score 593.5072357079856 qloss 1.1882546 ploss -1.918251\n",
      "ac [[ 0.9999352   0.9999985  -0.0104217  -0.7622713   0.90083605  0.99993324]] ua [[ 5.1693964   7.0629497  -0.01042208 -1.0016145   1.4766372   5.153574  ]]\n",
      "35 -th evaluation done. score: 4.150013897702649 step: 1000\n",
      "step 1000 score 229.11285554848646 qloss 1.2214949 ploss -2.4017506\n",
      "ac [[ 0.9080785   0.43572098 -0.23412274 -0.9384506  -0.09408811  0.31383854]] ua [[ 1.5164589   0.46693677 -0.23854688 -1.724903   -0.09436725  0.3247977 ]]\n",
      "36 -th evaluation done. score: 463.27136718589105 step: 1000\n",
      "step 1000 score 425.3863867596904 qloss 1.0552298 ploss -2.7737694\n",
      "ac [[ 0.31225073 -0.39583     0.14786345  0.9812508   0.00373438 -0.23772846]] ua [[ 0.32303736 -0.4186945   0.14895543  2.330169    0.00373439 -0.24236515]]\n",
      "37 -th evaluation done. score: 644.4041622485912 step: 1000\n",
      "step 1000 score 667.5091530630992 qloss 0.9297887 ploss -2.9811058\n",
      "ac [[ 0.6717279   0.431297    0.05363109 -0.9375129  -0.12510833 -0.02848952]] ua [[ 0.81388503  0.461489    0.0536826  -1.7171001  -0.12576728 -0.02849724]]\n",
      "38 -th evaluation done. score: 640.7968789886432 step: 1000\n",
      "step 1000 score 609.8111128990527 qloss 1.0151906 ploss -3.5855935\n",
      "ac [[-0.138922    0.27292603 -0.37760228  0.72510237 -0.07197171  0.5645086 ]] ua [[-0.13982621  0.28002265 -0.39726028  0.9183213  -0.07209638  0.63942605]]\n",
      "39 -th evaluation done. score: 874.9807415748264 step: 1000\n",
      "step 1000 score 723.111869436123 qloss 0.8528254 ploss -3.7521527\n",
      "ac [[-0.22803827  0.59106773  0.0683412  -0.99632365  0.02002762 -0.05304429]] ua [[-0.23211914  0.67930555  0.0684479  -3.1485631   0.0200303  -0.05309413]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 -th evaluation done. score: 960.4008050909816 step: 1000\n",
      "step 1000 score 912.7718689559412 qloss 1.078 ploss -4.818863\n",
      "ac [[-0.33776203  0.41144335  0.5903354   0.00706958  0.37996176  0.6100952 ]] ua [[-0.3515642   0.43734744  0.6781807   0.0070697   0.400015    0.70907295]]\n",
      "41 -th evaluation done. score: 1024.4529211297934 step: 1000\n",
      "step 1000 score 1127.0518556994955 qloss 0.9329557 ploss -5.1634927\n",
      "ac [[-0.00890831  0.8877228  -0.56323296  0.88511306  0.08787975 -0.4137287 ]] ua [[-0.00890854  1.4110781  -0.6375556   1.3988972   0.08810705 -0.44010168]]\n",
      "42 -th evaluation done. score: 1463.5167921435902 step: 1000\n",
      "step 1000 score 1194.494143120443 qloss 1.2799394 ploss -5.95556\n",
      "ac [[-0.9478713   0.92847425 -0.85919565  0.9752026   0.7163982   0.6220923 ]] ua [[-1.8103881   1.6472132  -1.2902639   2.1888433   0.90020597  0.728411  ]]\n",
      "43 -th evaluation done. score: 1372.2830078806494 step: 1000\n",
      "step 1000 score 1382.3558988798009 qloss 1.3343854 ploss -7.439957\n",
      "ac [[-0.7909996   0.97098553 -0.30892256  0.49378502  0.9591736  -0.22471215]] ua [[-1.0740963   2.109248   -0.31935388  0.5410535   1.9354761  -0.22861338]]\n",
      "44 -th evaluation done. score: 1656.410026159537 step: 1000\n",
      "step 1000 score 1616.2289278713552 qloss 1.0652249 ploss -7.0622535\n",
      "ac [[-0.4404725   0.8510911  -0.9293535  -0.07437938  0.20727     0.35844818]] ua [[-0.4728169   1.2600977  -1.653626   -0.07451701  0.21031713  0.37510413]]\n",
      "45 -th evaluation done. score: 1593.1835289871856 step: 1000\n",
      "step 1000 score 1794.1086959789266 qloss 1.2599189 ploss -7.9522266\n",
      "ac [[ 0.8211438  -0.9997314   0.40352646  0.99917114 -0.9222465  -0.361101  ]] ua [[ 1.1603187  -4.457664    0.42785418  3.894084   -1.6038532  -0.37815142]]\n",
      "46 -th evaluation done. score: 1526.544052687363 step: 1000\n",
      "step 1000 score 1578.3510464894212 qloss 1.7557716 ploss -9.30322\n",
      "ac [[-0.7812127  -0.9999408  -0.87925416  0.9999999  -0.99166954 -0.59116066]] ua [[-1.0484747 -5.214483  -1.3724711  8.286784  -2.7384033 -0.6794483]]\n",
      "47 -th evaluation done. score: 1821.3416970586363 step: 1000\n",
      "step 1000 score 1704.5376157540493 qloss 1.7595165 ploss -9.114167\n",
      "ac [[-0.7704116   0.9977694  -0.993827   -0.999283    0.86865246 -0.99364114]] ua [[-1.0213397  3.3987713 -2.8888078 -3.9666808  1.3275632 -2.873941 ]]\n",
      "48 -th evaluation done. score: 1742.8690916206258 step: 1000\n",
      "step 1000 score 1877.5024554825975 qloss 1.743345 ploss -8.782953\n",
      "ac [[-0.92108166  0.9280716  -0.9800749  -0.9998672   0.99096555  0.6712385 ]] ua [[-1.5961149  1.6443025 -2.2994542 -4.809632   2.6976647  0.8129938]]\n",
      "49 -th evaluation done. score: 2160.923237156616 step: 1000\n",
      "step 1000 score 2249.5875577556744 qloss 2.3903751 ploss -10.860293\n",
      "ac [[ 0.99683267 -0.9990698   0.9471403   0.9999998  -0.9994905  -0.9577428 ]] ua [[ 3.2232225 -3.836449   1.8032382  8.60314   -4.137559  -1.9178866]]\n",
      "50 -th evaluation done. score: 2186.815495415543 step: 1000\n",
      "step 1000 score 2313.850275023378 qloss 2.287147 ploss -14.885573\n",
      "ac [[0.8224964  0.99001056 0.8608769  0.970863   0.9304509  0.96709484]] ua [[1.1644856 2.6471872 1.2967223 2.1071098 1.6617383 2.0453422]]\n",
      "51 -th evaluation done. score: 2505.1858411315848 step: 1000\n",
      "step 1000 score 2452.784707360338 qloss 1.6140597 ploss -14.752404\n",
      "ac [[0.90284616 0.96695155 0.9993475  0.41286978 0.995515   0.995099  ]] ua [[1.4874051 2.0431318 4.0136886 0.4390658 3.0489802 3.0045135]]\n",
      "52 -th evaluation done. score: 2631.3220020663493 step: 1000\n",
      "step 1000 score 2532.553802485302 qloss 2.0043137 ploss -15.120526\n",
      "ac [[ 0.7159153   0.00771301  0.65937674  0.95219564  0.13167624 -0.25313553]] ua [[ 0.89921474  0.00771317  0.7917102   1.8547964   0.1324453  -0.2587602 ]]\n",
      "53 -th evaluation done. score: 2464.1648365308206 step: 1000\n",
      "step 1000 score 2579.2849737266392 qloss 3.7140512 ploss -17.562735\n",
      "ac [[-0.66993654  0.6231937  -0.9641474  -0.9996501   0.92664665  0.8793387 ]] ua [[-0.81062806  0.73020977 -2.0016985  -4.3255453   1.6341238   1.3728439 ]]\n",
      "54 -th evaluation done. score: 2377.9923126206645 step: 1000\n",
      "step 1000 score 2525.860356677259 qloss 2.41963 ploss -20.699821\n",
      "ac [[-0.9927008   0.9995386  -0.9999122  -0.99963325  0.92062026  0.7121833 ]] ua [[-2.8047392   4.187055   -5.016879   -4.3020043   1.59308     0.89160043]]\n",
      "55 -th evaluation done. score: 2096.5171334751517 step: 1000\n",
      "step 1000 score 2263.959373137767 qloss 3.2627678 ploss -19.15405\n",
      "ac [[-0.9584402   0.9997992  -0.99997985  0.6241098   0.95318925 -0.9205286 ]] ua [[-1.9263843  4.6031737 -5.7501793  0.731709   1.8655542 -1.592479 ]]\n",
      "56 -th evaluation done. score: 2780.4682906548596 step: 1000\n",
      "step 1000 score 2788.4151843332493 qloss 2.3872802 ploss -18.348877\n",
      "ac [[-0.69972056  0.99993503 -0.35232458  0.94797623  0.90520734 -0.88225466]] ua [[-0.8667526   5.1669216  -0.36809534  1.8114231   1.500327   -1.3858507 ]]\n",
      "57 -th evaluation done. score: 2012.9596102374492 step: 1000\n",
      "step 1000 score 1328.1219873913365 qloss 2.6825798 ploss -21.842829\n",
      "ac [[ 0.9999989   0.98532706 -0.4585205  -0.12446424  0.01958016  0.04876812]] ua [[ 7.235111    2.4537656  -0.49543637 -0.12511301  0.01958267  0.04880685]]\n",
      "58 -th evaluation done. score: 2295.897889922716 step: 1000\n",
      "step 1000 score 1236.9266370539704 qloss 4.0123844 ploss -23.556818\n",
      "ac [[ 0.9999966   0.96047336 -0.5124666   0.7013581  -0.27749863  0.40344992]] ua [[ 6.653491   1.9519842 -0.5660692  0.8699684 -0.28497    0.4277628]]\n",
      "59 -th evaluation done. score: 1859.969730736839 step: 1000\n",
      "step 1000 score 2459.077123127167 qloss 3.6563077 ploss -22.146944\n",
      "ac [[ 0.9997831  -0.96515906  0.9999975  -0.925663   -0.9999179   0.99908113]] ua [[ 4.564534  -2.0162683  6.7627664 -1.6272084 -5.0509596  3.8425333]]\n",
      "60 -th evaluation done. score: 1029.7243217559212 step: 1000\n",
      "step 1000 score 2959.903730754584 qloss 3.4115753 ploss -24.983803\n",
      "ac [[-0.9606095   0.99113315 -0.9929809  -0.18763874  0.03671902 -0.69929516]] ua [[-1.9537438   2.7070727  -2.8243713  -0.18988863  0.03673553 -0.86591965]]\n",
      "61 -th evaluation done. score: 3439.4454330707426 step: 1000\n",
      "step 1000 score 1028.3282268230334 qloss 3.2027936 ploss -25.3776\n",
      "ac [[ 0.99998504  1.         -0.17877868 -0.6140496   0.05590482  0.5238876 ]] ua [[ 5.9025574  12.044431   -0.18072075 -0.71539634  0.05596318  0.5816829 ]]\n",
      "62 -th evaluation done. score: 3402.9335616780168 step: 1000\n",
      "step 1000 score 3281.321425107295 qloss 8.886215 ploss -29.466726\n",
      "ac [[-0.20464261 -0.38573906  0.6623897  -0.82961863 -0.9995598  -0.86328477]] ua [[-0.20757332 -0.40678453  0.79705954 -1.1869118  -4.2106123  -1.3060982 ]]\n",
      "63 -th evaluation done. score: 3423.0167146257945 step: 1000\n",
      "step 1000 score 3382.4717609910867 qloss 8.322168 ploss -31.869955\n",
      "ac [[ 0.61405295 -0.03273837  0.35678998 -0.9285987  -0.99999934 -0.5973161 ]] ua [[ 0.71540165 -0.03275007  0.3732028  -1.6481166  -7.539427   -0.68896407]]\n",
      "64 -th evaluation done. score: 3336.661854488817 step: 1000\n",
      "step 1000 score 3470.8277163192356 qloss 4.9220037 ploss -27.788786\n",
      "ac [[0.06931935 0.99933416 0.9997557  0.38245553 0.6535117  0.628623  ]] ua [[0.06943071 4.00371    4.5051985  0.40293273 0.78140366 0.7391362 ]]\n",
      "65 -th evaluation done. score: 3330.106620493729 step: 1000\n",
      "step 1000 score 3539.247427319948 qloss 4.7504196 ploss -30.574179\n",
      "ac [[ 0.99779993 -0.94574517  0.99997324 -0.17903389 -0.99999905  0.9921876 ]] ua [[ 3.4056854  -1.7898543   5.607331   -0.18098444 -7.2173476   2.7706394 ]]\n",
      "66 -th evaluation done. score: 3567.2521932492664 step: 1000\n",
      "step 1000 score 3528.4562601967955 qloss 5.744135 ploss -32.453617\n",
      "ac [[0.7037825  0.9982971  1.         0.36975867 0.9989122  0.99377656]] ua [[ 0.874756   3.5339015 10.631344   0.3881435  3.7581224  2.884735 ]]\n",
      "67 -th evaluation done. score: 3669.5392096846017 step: 1000\n",
      "step 1000 score 3904.571383773768 qloss 7.9288573 ploss -41.364536\n",
      "ac [[-0.9954987   1.         -1.          0.32907054  0.98811513 -0.9663694 ]] ua [[ -3.0471437    9.176531   -12.854947     0.34178558   2.5598388\n",
      "   -2.0342546 ]]\n",
      "68 -th evaluation done. score: 3658.7471122771085 step: 1000\n",
      "step 1000 score 3982.985865286351 qloss 6.7638454 ploss -36.733047\n",
      "ac [[-0.9469778   0.8714048  -0.99920005 -0.9460334  -0.99999976 -0.66752696]] ua [[-1.8016615   1.3388879  -3.9119005  -1.7925913  -8.20683    -0.80626893]]\n",
      "69 -th evaluation done. score: 2124.411870537948 step: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000 score 3821.625463372008 qloss 9.201505 ploss -42.082973\n",
      "ac [[-0.99963266  0.9990637  -0.99999994 -0.9987599  -1.         -0.7592586 ]] ua [[ -4.300975     3.8330698   -8.6942835   -3.6925879  -11.136753\n",
      "   -0.99446213]]\n",
      "70 -th evaluation done. score: 3607.2219537116084 step: 1000\n",
      "step 1000 score 3812.395383891117 qloss 11.045296 ploss -38.880157\n",
      "ac [[-0.9912344   1.         -1.          0.83978254  0.961404   -0.97832245]] ua [[-2.7128406  9.762374  -9.558972   1.220435   1.9641341 -2.2568636]]\n",
      "71 -th evaluation done. score: 4342.646623332922 step: 1000\n",
      "step 1000 score 3589.848222439263 qloss 9.379164 ploss -46.809155\n",
      "ac [[ 0.9912324  -0.57086515  0.99999994 -0.6016259  -0.963103    0.99948967]] ua [[ 2.7127213 -0.6488053  8.761921  -0.6956916 -1.9870768  4.136655 ]]\n",
      "72 -th evaluation done. score: 3793.224555304147 step: 1000\n",
      "step 1000 score 3921.604060847143 qloss 6.860692 ploss -48.266235\n",
      "ac [[0.88121545 0.9999934  1.         0.75408906 0.9999964  0.999942  ]] ua [[ 1.3811809   6.3121986  15.197865    0.98236763  6.625395    5.2233715 ]]\n",
      "73 -th evaluation done. score: 3873.277168777844 step: 1000\n",
      "step 1000 score 3819.741101176588 qloss 7.2665215 ploss -52.071095\n",
      "ac [[-0.80173653  1.         -1.          0.8005546   0.98409265 -0.8980722 ]] ua [[ -1.1034547   8.567959  -13.440245    1.1001548   2.4130676  -1.4621648]]\n",
      "74 -th evaluation done. score: 4093.93935316375 step: 1000\n",
      "step 1000 score 4055.849247079174 qloss 7.001487 ploss -50.251343\n",
      "ac [[ 1.         -0.98868775  1.         -0.98371464 -1.          0.9999702 ]] ua [[ 10.660806   -2.5846748  39.470524   -2.40123   -10.453999    5.5577006]]\n",
      "75 -th evaluation done. score: 3973.8294444002904 step: 1000\n",
      "step 1000 score 4148.030588967228 qloss 8.440329 ploss -50.168514\n",
      "ac [[ 0.02596473  0.99842674  1.         -0.46345612  0.9914633   0.8900226 ]] ua [[ 0.02597057  3.5734627  12.892771   -0.5017039   2.7261212   1.4220341 ]]\n",
      "76 -th evaluation done. score: 3477.904425266334 step: 1000\n",
      "step 1000 score 4321.846148020057 qloss 10.775472 ploss -51.86566\n",
      "ac [[-0.9786898  1.        -1.         0.6695792  0.996657  -0.9796369]] ua [[ -2.2655     12.901811  -13.032355    0.80998     3.1961703  -2.2884715]]\n",
      "77 -th evaluation done. score: 4754.607598485331 step: 1000\n",
      "step 1000 score 4366.039181748287 qloss 9.854377 ploss -61.892895\n",
      "ac [[ 1.         -0.99970365  1.         -0.9955759  -0.9997388   0.9997165 ]] ua [[18.468212  -4.408428  17.585506  -3.0558043 -4.4715495  4.430801 ]]\n",
      "78 -th evaluation done. score: 3932.7528047705728 step: 1000\n",
      "step 1000 score 4348.59482249143 qloss 9.951107 ploss -62.05676\n",
      "ac [[ 0.9999999  -0.99977195  1.         -0.99782777 -0.9999993   0.99999726]] ua [[ 8.206575  -4.539453  18.050468  -3.4120154 -7.361829   6.7311187]]\n",
      "79 -th evaluation done. score: 4873.321492916364 step: 1000\n",
      "step 1000 score 3261.7684177388865 qloss 8.802535 ploss -61.694386\n",
      "ac [[ 0.21759348  0.85811025  0.7598448   1.         -0.39264855  0.11178732]] ua [[ 0.22112861  1.2861322   0.99584746 12.585262   -0.41492754  0.1122565 ]]\n",
      "80 -th evaluation done. score: 4205.842469839049 step: 1000\n",
      "step 1000 score 4330.368481941559 qloss 11.062691 ploss -60.321697\n",
      "ac [[-0.9999459   0.99995905 -1.         -0.9997329  -0.99999946  0.34851736]] ua [[ -5.258359     5.3986454  -14.719259    -4.4605055   -7.617173\n",
      "    0.36375517]]\n",
      "81 -th evaluation done. score: 3703.836089835541 step: 1000\n",
      "step 1000 score 4447.441506179848 qloss 11.747883 ploss -63.536366\n",
      "ac [[ 1.         -0.97477746  1.         -0.9997449  -0.99999994  0.9926667 ]] ua [[ 8.761531  -2.1802373 19.252888  -4.483495  -8.789502   2.8024004]]\n",
      "82 -th evaluation done. score: 4456.284391999364 step: 1000\n",
      "step 1000 score 4382.572149019067 qloss 12.091127 ploss -64.49604\n",
      "ac [[0.32860085 0.99999744 0.99999964 0.40898192 1.         0.99768037]] ua [[ 0.34125894  6.763319    7.6849256   0.43438804 10.259086    3.3791876 ]]\n",
      "83 -th evaluation done. score: 4527.429558813336 step: 1000\n",
      "step 1000 score 4576.8532058121755 qloss 16.966198 ploss -73.348366\n",
      "ac [[-0.99899256  0.9962804  -0.999934   -0.9954338  -0.99935955  0.5247259 ]] ua [[-3.7965937  3.1427038 -5.1596026 -3.0399613 -4.023172   0.5828392]]\n",
      "84 -th evaluation done. score: 3983.340919569718 step: 1000\n",
      "step 1000 score 4514.057065970753 qloss 9.811756 ploss -72.277565\n",
      "ac [[-0.926384    0.98419005 -1.         -0.9999882  -1.          0.7223861 ]] ua [[-1.6322685  2.4161599 -9.528684  -6.020669  -9.892874   0.9126172]]\n",
      "85 -th evaluation done. score: 4801.003827762259 step: 1000\n",
      "step 1000 score 4573.405605070165 qloss 14.537273 ploss -73.49391\n",
      "ac [[ 0.9999999  -0.98803425  1.         -0.9999509  -0.99999756  0.9978854 ]] ua [[ 8.076273  -2.556425  24.752005  -5.3066354 -6.7970743  3.42551  ]]\n",
      "86 -th evaluation done. score: 4899.489658938962 step: 1000\n",
      "step 1000 score 4635.990328271777 qloss 10.689352 ploss -74.63715\n",
      "ac [[-0.99965495  0.9996111  -1.         -0.9999947  -0.99999666  0.3557043 ]] ua [[ -4.332528     4.2726994  -12.871459    -6.4206862   -6.6365614\n",
      "    0.37195933]]\n",
      "87 -th evaluation done. score: 4954.913990213599 step: 1000\n",
      "step 1000 score 4538.032835492713 qloss 17.439644 ploss -78.4127\n",
      "ac [[ 1.         -0.9929382   1.         -0.9875318  -0.9992518   0.99924636]] ua [[10.908169  -2.8213353 29.313334  -2.5357275 -3.945489   3.9416685]]\n",
      "88 -th evaluation done. score: 5201.783473064836 step: 1000\n",
      "step 1000 score 4707.483937236005 qloss 22.754585 ploss -85.47852\n",
      "ac [[ 1.         -0.96171504  1.         -0.99997234 -0.9999998   0.9999227 ]] ua [[17.694754  -1.9682586 46.702408  -5.594493  -7.9821787  5.08098  ]]\n",
      "89 -th evaluation done. score: 4876.727045925762 step: 1000\n",
      "step 1000 score 4465.933448356457 qloss 15.778002 ploss -84.28423\n",
      "ac [[0.29944563 1.         1.         0.6909485  0.99999505 0.9733324 ]] ua [[ 0.30891052 17.345795   14.786655    0.84976846  6.457818    2.1520157 ]]\n",
      "90 -th evaluation done. score: 4533.730824426442 step: 1000\n",
      "step 1000 score 4802.013537103497 qloss 12.327776 ploss -87.89919\n",
      "ac [[0.69458264 0.9999932  1.         0.59638345 0.9999924  0.9985849 ]] ua [[ 0.85675615  6.2954903  17.082989    0.6875154   6.241624    3.6265419 ]]\n",
      "91 -th evaluation done. score: 5056.525526631071 step: 1000\n",
      "step 1000 score 4711.863587914229 qloss 18.78258 ploss -95.87827\n",
      "ac [[ 1.         -0.9900456   1.         -0.99926746 -0.9999977   0.9992814 ]] ua [[16.792847  -2.6489527 38.26861   -3.9559457 -6.7989693  3.9655402]]\n",
      "92 -th evaluation done. score: 4973.2635938211415 step: 1000\n",
      "step 1000 score 4794.077288795115 qloss 14.604546 ploss -95.5662\n",
      "ac [[-0.9999992   0.9999884  -1.         -0.99999195 -0.9999901   0.5220837 ]] ua [[ -7.3752136    6.019881   -22.092213    -6.213354    -6.102072\n",
      "    0.57919997]]\n",
      "93 -th evaluation done. score: 5018.262857171407 step: 1000\n",
      "step 1000 score 4574.218437876407 qloss 15.79987 ploss -96.38854\n",
      "ac [[0.6800291  1.         1.         0.8519975  0.99999803 0.9967902 ]] ua [[ 0.82916814 11.148583   12.557047    1.2633951   6.895583    3.2165172 ]]\n",
      "94 -th evaluation done. score: 4640.350153656449 step: 1000\n",
      "step 1000 score 4751.04941346606 qloss 20.698479 ploss -108.2429\n",
      "ac [[ 1.         -0.99927783  1.         -0.99972147 -0.99999756  0.7570833 ]] ua [[24.124561   -3.9631145  46.445713   -4.4395523  -6.772711    0.98934585]]\n",
      "95 -th evaluation done. score: 4484.992564474734 step: 1000\n",
      "step 1000 score 4759.348822859227 qloss 17.672482 ploss -99.35221\n",
      "ac [[-0.8424505  1.        -1.         0.9927837  0.9999006 -0.9835491]] ua [[ -1.229556   37.96407   -21.101078    2.8104706   4.954747   -2.3961284]]\n",
      "96 -th evaluation done. score: 5023.745961647578 step: 1000\n",
      "step 1000 score 4656.238726584136 qloss 26.159851 ploss -106.628174\n",
      "ac [[0.6104814 1.        1.        0.79036   0.9992255 0.9993591]] ua [[ 0.7096883 15.856379  27.461689   1.07239    3.928061   4.022796 ]]\n",
      "97 -th evaluation done. score: 4994.387203015314 step: 1000\n",
      "step 1000 score 4898.887860637319 qloss 11.428053 ploss -106.90695\n",
      "ac [[ 1.        -0.999636   1.        -0.9996797 -0.9986377  0.9724138]] ua [[15.784539  -4.305639  43.1729    -4.369641  -3.6455557  2.1348484]]\n",
      "98 -th evaluation done. score: 4757.460116628939 step: 1000\n",
      "step 1000 score 4806.081071327969 qloss 14.640841 ploss -107.075714\n",
      "ac [[0.4750055  1.         1.         0.83182865 1.         0.9999827 ]] ua [[ 0.51651466 14.613476   17.939451    1.1940434   8.984886    5.825974  ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 -th evaluation done. score: 2297.655465695924 step: 1000\n",
      "step 1000 score 4926.087021750739 qloss 19.142403 ploss -108.53054\n",
      "ac [[-0.7501885  1.        -1.         0.9971778  0.9999835 -0.9999768]] ua [[ -0.97338617  28.971859   -26.440825     3.2809892    5.8502283\n",
      "   -5.6845694 ]]\n",
      "100 -th evaluation done. score: 5057.526173680629 step: 1000\n",
      "step 1000 score 4821.083478712487 qloss 23.093296 ploss -108.90926\n",
      "ac [[-0.9359721   1.         -1.          0.20133498  0.96691656 -0.99982923]] ua [[ -1.7045232   23.09488    -22.850708     0.20412357   2.042595\n",
      "   -4.684446  ]]\n",
      "101 -th evaluation done. score: 5142.822478239042 step: 1000\n",
      "step 1000 score 5104.132212288603 qloss 16.943672 ploss -114.6103\n",
      "ac [[-0.99999976  0.999988   -1.         -0.99999577 -0.9999781   0.8373935 ]] ua [[ -7.922332    6.0152493 -21.44712    -6.5226088  -5.7107496   1.2123852]]\n",
      "102 -th evaluation done. score: 5251.929790100731 step: 1000\n",
      "step 1000 score 4746.261001140177 qloss 19.579079 ploss -111.61233\n",
      "ac [[-0.08260129  1.          1.          0.2964868   0.99989736  0.99999404]] ua [[-0.08278993 10.070742   32.08756     0.3056634   4.9384847   6.367575  ]]\n",
      "103 -th evaluation done. score: 5222.2033200936185 step: 1000\n",
      "step 1000 score 5026.623234263484 qloss 23.50403 ploss -123.097015\n",
      "ac [[ 1.         -0.9996968   1.         -0.99999815 -0.99998087  0.99912536]] ua [[21.4053    -4.3971252 39.221035  -6.899235  -5.776951   3.867268 ]]\n",
      "104 -th evaluation done. score: 5169.538022197486 step: 1000\n",
      "step 1000 score 5172.514770845054 qloss 16.404928 ploss -116.74735\n",
      "ac [[ 1.         -0.9999847   1.         -0.9999456  -0.99986696  0.9999914 ]] ua [[16.250177  -5.8919454 34.609703  -5.255882  -4.808825   6.189623 ]]\n",
      "105 -th evaluation done. score: 4776.883372657225 step: 1000\n",
      "step 1000 score 4709.801104186201 qloss 20.350212 ploss -130.9499\n",
      "ac [[0.7763701  1.         1.         0.58477104 0.9999986  0.9999504 ]] ua [[ 1.0361673  17.159416   25.969755    0.66968256  7.108067    5.30198   ]]\n",
      "106 -th evaluation done. score: 5235.821519079819 step: 1000\n",
      "step 1000 score 5277.734238751097 qloss 20.671593 ploss -147.31615\n",
      "ac [[-0.98706675  1.         -1.         -0.9215281   0.99485856 -0.99999905]] ua [[ -2.5173087  35.940437  -29.493725   -1.5990678   2.9805005  -7.295556 ]]\n",
      "107 -th evaluation done. score: 5343.778319795892 step: 1000\n",
      "step 1000 score 4784.989994411467 qloss 21.872425 ploss -132.9738\n",
      "ac [[ 1.         -0.99962276  1.         -0.999945   -0.99999833  0.9999993 ]] ua [[16.309698  -4.287668  44.56713   -5.2499676 -6.957713   7.4210634]]\n",
      "108 -th evaluation done. score: 5283.435491589585 step: 1000\n",
      "step 1000 score 5089.534880381221 qloss 21.702219 ploss -136.44644\n",
      "ac [[ 1.         -0.99998546  1.         -0.985251   -0.99998045  1.        ]] ua [[22.470865  -5.9172454 55.844707  -2.45116   -5.7685103  9.709257 ]]\n",
      "109 -th evaluation done. score: 5196.212566239576 step: 1000\n",
      "step 1000 score 5231.893070721739 qloss 17.008095 ploss -143.50928\n",
      "ac [[ 1.         -0.9999988   1.         -0.99999356 -0.99999803 -0.71615744]] ua [[25.012398  -7.1861777 43.877205  -6.321861  -6.9093037 -0.8997116]]\n",
      "110 -th evaluation done. score: 5101.039333183502 step: 1000\n",
      "step 1000 score 5240.73806242004 qloss 18.123514 ploss -140.35979\n",
      "ac [[0.03117579 1.         1.         0.41323486 0.9999989  0.99982005]] ua [[ 0.0311859  15.48041    24.778555    0.43950596  7.1619477   4.657686  ]]\n",
      "111 -th evaluation done. score: 5484.050181477624 step: 1000\n",
      "step 1000 score 5375.9194111919505 qloss 25.422737 ploss -156.58603\n",
      "ac [[-0.96629536  1.         -1.          0.99874145  1.         -0.9999646 ]] ua [[ -2.0331366  28.117058  -11.371668    3.6851249  10.46696    -5.471388 ]]\n",
      "112 -th evaluation done. score: 5585.38165557629 step: 1000\n",
      "step 1000 score 5185.97642494132 qloss 24.77976 ploss -144.25069\n",
      "ac [[-0.99997175  0.9999989  -1.         -0.9998087  -1.          0.8688411 ]] ua [[ -5.5817637   7.1744404 -21.318064   -4.6274266 -12.919693    1.3283322]]\n",
      "113 -th evaluation done. score: 5397.270607126952 step: 1000\n",
      "step 1000 score 5142.02465371873 qloss 29.01728 ploss -153.5666\n",
      "ac [[-0.99981105  0.99995124 -1.         -0.9984001  -1.          0.48216757]] ua [[ -4.633354    5.3115325 -18.540533   -3.5650716 -10.084759    0.5258046]]\n",
      "114 -th evaluation done. score: 5189.479080635629 step: 1000\n",
      "step 1000 score 5268.838651210824 qloss 23.808079 ploss -158.19801\n",
      "ac [[ 1.        -0.9999999  1.        -0.9999556 -1.         1.       ]] ua [[ 42.94467    -8.350081   46.36753    -5.3580976 -10.708813   10.860721 ]]\n",
      "115 -th evaluation done. score: 5435.088103516967 step: 1000\n",
      "step 1000 score 5213.80011594848 qloss 31.467884 ploss -167.31636\n",
      "ac [[-0.99999875  0.9999465  -1.         -1.         -1.          0.7873682 ]] ua [[ -7.118963    5.2650723 -26.952423  -11.440595  -18.719341    1.0644686]]\n",
      "116 -th evaluation done. score: 5495.847093333764 step: 1000\n",
      "step 1000 score 5200.1918964357355 qloss 28.938206 ploss -169.33102\n",
      "ac [[ 0.99999994 -0.9409336   1.         -0.9999958  -0.99999994  0.9389674 ]] ua [[ 8.682417  -1.7461314 28.916933  -6.5479884 -8.809093   1.7292509]]\n",
      "117 -th evaluation done. score: 5124.753440495595 step: 1000\n",
      "step 1000 score 5326.769324400654 qloss 19.16658 ploss -169.73524\n",
      "ac [[-0.99999255  0.99999607 -1.         -0.9999999  -1.          0.3526251 ]] ua [[ -6.242589     6.5588484  -17.307611    -8.330804   -12.843766\n",
      "    0.36843848]]\n",
      "118 -th evaluation done. score: 5342.145283531599 step: 1000\n",
      "step 1000 score 5263.429168700502 qloss 28.025368 ploss -171.59819\n",
      "ac [[0.8650922  1.         1.         0.66609544 1.         0.9999996 ]] ua [[ 1.3132367  29.51506    35.401768    0.80369145 13.754452    7.5406194 ]]\n",
      "119 -th evaluation done. score: 5510.640184823017 step: 1000\n",
      "step 1000 score 5288.805861926544 qloss 44.492203 ploss -172.72339\n",
      "ac [[-0.8418147   1.         -1.          0.9985243   1.         -0.99999964]] ua [[ -1.2273698  37.61649   -40.22617     3.6055088  14.216845   -7.708934 ]]\n",
      "120 -th evaluation done. score: 5482.369609369751 step: 1000\n",
      "step 1000 score 5276.950063142886 qloss 21.89035 ploss -183.77156\n",
      "ac [[ 1.        -1.         1.        -0.9999655 -1.         1.       ]] ua [[ 38.817383  -9.30291   44.61843   -5.481475 -11.821017  10.621712]]\n",
      "121 -th evaluation done. score: 5529.188894688377 step: 1000\n",
      "step 1000 score 5324.221955556595 qloss 19.552708 ploss -192.97173\n",
      "ac [[ 0.05740887  1.          1.         -0.03915847  0.99998176  0.99999964]] ua [[ 5.7472073e-02  3.1835970e+01  4.7869614e+01 -3.9178502e-02\n",
      "   5.8011885e+00  7.6480031e+00]]\n",
      "122 -th evaluation done. score: 5586.402105855853 step: 1000\n",
      "step 1000 score 5459.710457054762 qloss 27.166847 ploss -193.80998\n",
      "ac [[ 1.        -0.9999979  1.        -0.999825  -0.9999926  1.       ]] ua [[31.526194  -6.8553977 54.70987   -4.671874  -6.2502265 14.805904 ]]\n",
      "123 -th evaluation done. score: 5628.114851551396 step: 1000\n",
      "step 1000 score 5353.700978274827 qloss 26.126095 ploss -196.34023\n",
      "ac [[ 1.        -0.9999881  1.        -0.9999685 -0.9999999  1.       ]] ua [[17.616882 -6.006739 51.820927 -5.528976 -8.427848 13.649805]]\n",
      "124 -th evaluation done. score: 5492.637824088137 step: 1000\n",
      "step 1000 score 5406.872360343807 qloss 31.299553 ploss -203.80879\n",
      "ac [[-0.95000887  1.         -1.          0.99975836  1.         -1.        ]] ua [[ -1.8318727  40.541405  -36.916836    4.5106287  10.79416   -13.658321 ]]\n",
      "125 -th evaluation done. score: 5427.287269207981 step: 1000\n",
      "step 1000 score 5536.604129311231 qloss 32.872322 ploss -199.06306\n",
      "ac [[ 1.        -1.         1.        -0.3595044 -0.9999998  1.       ]] ua [[30.426222  -9.323351  33.728104  -0.3763166 -7.958813   9.9614525]]\n",
      "126 -th evaluation done. score: 5104.702953805843 step: 1000\n",
      "step 1000 score 5452.169406473688 qloss 19.829926 ploss -217.90276\n",
      "ac [[ 1.         -0.99985194  1.         -0.9996188  -1.          1.        ]] ua [[31.504065 -4.755172 54.593193 -4.282583 -9.041032 17.169273]]\n",
      "127 -th evaluation done. score: 5592.179122651189 step: 1000\n",
      "step 1000 score 5423.071519356262 qloss 26.563232 ploss -214.73521\n",
      "ac [[ 1.         -0.9999999   1.         -0.99999744 -1.          1.        ]] ua [[ 37.056965  -8.248615  60.420788  -6.769037 -10.882982  11.366635]]\n",
      "128 -th evaluation done. score: 5496.948084141509 step: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000 score 5526.155365974952 qloss 24.960056 ploss -222.4532\n",
      "ac [[-0.9999143   0.9999993  -1.         -0.9999995  -1.          0.33748168]] ua [[ -5.029354    7.3571315 -15.688109   -7.5892105 -15.367802    0.3512478]]\n",
      "129 -th evaluation done. score: 5499.6646495364075 step: 1000\n",
      "step 1000 score 5567.662731760959 qloss 37.33536 ploss -229.11\n",
      "ac [[-0.20625788  1.          1.          0.7867939   1.          0.9999992 ]] ua [[-0.20925978 41.693035   26.947708    1.0629593  13.379168    7.4395638 ]]\n",
      "130 -th evaluation done. score: 5732.743146696048 step: 1000\n",
      "step 1000 score 5273.843695816832 qloss 32.79429 ploss -230.00787\n",
      "ac [[-1.          0.99999785 -1.         -0.99999815 -1.         -0.4166708 ]] ua [[ -8.140221    6.857225  -30.698286   -6.9239516 -20.976667   -0.4436566]]\n",
      "131 -th evaluation done. score: 4536.117146932496 step: 1000\n",
      "step 1000 score 5580.103337331154 qloss 61.466263 ploss -235.69005\n",
      "ac [[-0.9885276  1.        -1.         0.9961969  1.        -1.       ]] ua [[ -2.5776005  39.598705  -35.896523    3.1315842  11.50828   -17.709528 ]]\n",
      "132 -th evaluation done. score: 5487.560272625845 step: 1000\n",
      "step 1000 score 5451.960342659032 qloss 24.854053 ploss -238.3962\n",
      "ac [[ 1.         -1.          1.         -0.99989754 -1.          1.        ]] ua [[ 28.816547   -9.964536   44.217365   -4.9395146 -12.1790285  13.20991  ]]\n",
      "133 -th evaluation done. score: 5318.618883824213 step: 1000\n",
      "step 1000 score 5364.051623846998 qloss 57.320087 ploss -241.40356\n",
      "ac [[ 1.         -1.          1.         -0.99996465 -1.          1.        ]] ua [[ 55.398018  -9.064005  53.33022   -5.470857 -13.664785  15.670382]]\n",
      "134 -th evaluation done. score: 5379.731836564919 step: 1000\n",
      "step 1000 score 5466.656010550648 qloss 24.21072 ploss -253.04414\n",
      "ac [[-0.9999931   0.99999976 -1.         -0.99999976 -1.          0.5295964 ]] ua [[ -6.2890406    8.03824    -20.188854    -8.231895   -15.291716\n",
      "    0.58958405]]\n",
      "135 -th evaluation done. score: 5639.722945073379 step: 1000\n",
      "step 1000 score 5468.808196276669 qloss 62.553436 ploss -262.7446\n",
      "ac [[ 0.71892756  1.          0.9999916  -0.72672284  0.99975663 -0.9891064 ]] ua [[ 0.9054216 15.535509   6.193776  -0.921747   4.506992  -2.6036308]]\n",
      "136 -th evaluation done. score: 5417.402394053836 step: 1000\n",
      "step 1000 score 5533.862124442533 qloss 35.282127 ploss -258.13885\n",
      "ac [[-0.9992534   0.9999901  -1.         -0.99998295 -1.          0.31187662]] ua [[ -3.9463768    6.102522   -24.200901    -5.8385806  -19.239761\n",
      "    0.32262293]]\n",
      "137 -th evaluation done. score: 5597.074012990593 step: 1000\n",
      "step 1000 score 5590.707653345943 qloss 31.675175 ploss -261.26303\n",
      "ac [[-0.99775904  1.         -1.          0.9897916   1.         -0.99999255]] ua [[ -3.3964467  29.560877  -35.55664     2.6362858  10.604593   -6.2635965]]\n",
      "138 -th evaluation done. score: 5511.89562206222 step: 1000\n",
      "step 1000 score 5605.546742772772 qloss 28.283184 ploss -269.01434\n",
      "ac [[-1.          1.         -1.         -0.99985534 -0.988471   -1.        ]] ua [[-12.388453   40.11065   -59.279694   -4.767342   -2.5751262  -9.704931 ]]\n",
      "139 -th evaluation done. score: 5696.330403053419 step: 1000\n",
      "step 1000 score 5501.718658821414 qloss 26.493721 ploss -259.02353\n",
      "ac [[-0.9999281   0.9999986  -1.         -0.99999917 -1.          0.6043994 ]] ua [[ -5.117742    7.085628  -39.05735    -7.3265495 -22.159433    0.7000497]]\n",
      "140 -th evaluation done. score: 5555.746245630126 step: 1000\n",
      "step 1000 score 5525.837194343264 qloss 26.99132 ploss -262.85693\n",
      "ac [[-0.9994198  1.        -1.         0.8373639  1.        -1.       ]] ua [[ -4.072561   49.618687  -34.240536    1.2122859   9.0577545 -14.640363 ]]\n",
      "141 -th evaluation done. score: 5476.846491769458 step: 1000\n",
      "step 1000 score 5644.297950570227 qloss 50.68493 ploss -268.24597\n",
      "ac [[-0.99697393  1.         -1.          0.9974346   1.         -1.        ]] ua [[ -3.2460556  46.178856  -33.770744    3.3287637  13.582432  -21.562933 ]]\n",
      "142 -th evaluation done. score: 5499.623162851583 step: 1000\n",
      "step 1000 score 5623.561663368942 qloss 31.30665 ploss -276.0508\n",
      "ac [[-0.99876267  1.         -1.          0.9990909   1.         -1.        ]] ua [[ -3.6936786  51.821957  -33.615845    3.8479733  23.582079  -26.122185 ]]\n",
      "143 -th evaluation done. score: 5480.669208577375 step: 1000\n",
      "step 1000 score 5459.413638564865 qloss 28.58569 ploss -275.4617\n",
      "ac [[-0.9999999   0.99999934 -1.         -0.9999355  -1.          0.16651657]] ua [[ -8.234996     7.5221634  -38.37684     -5.1700087  -25.018234\n",
      "    0.16808175]]\n",
      "144 -th evaluation done. score: 5728.206355707817 step: 1000\n",
      "step 1000 score 5427.605599640459 qloss 19.12326 ploss -281.74698\n",
      "ac [[-0.99999964  0.99999887 -1.         -0.9999998  -1.          0.39422607]] ua [[ -7.824646     7.146781   -32.495667    -8.665678   -15.548745\n",
      "    0.41679397]]\n",
      "145 -th evaluation done. score: 5621.78155935194 step: 1000\n",
      "step 1000 score 5670.739999439001 qloss 38.354538 ploss -286.61404\n",
      "ac [[-0.9919826  1.        -1.         0.9944102  1.        -1.       ]] ua [[ -2.7576334  64.03887   -38.320095    2.9385903  16.884037  -20.042345 ]]\n",
      "146 -th evaluation done. score: 5321.160353655936 step: 1000\n",
      "step 1000 score 5476.253704348542 qloss 36.07836 ploss -284.23102\n",
      "ac [[-0.999488   1.        -1.         0.9870593  1.        -1.       ]] ua [[ -4.135012   43.81546   -36.522575    2.5170145  14.85211   -33.56647  ]]\n",
      "147 -th evaluation done. score: 5698.004523327198 step: 1000\n",
      "step 1000 score 5645.818918958569 qloss 24.246048 ploss -294.68286\n",
      "ac [[-0.9998589  1.        -1.         0.9251572  1.        -1.       ]] ua [[ -4.779414   55.532608  -34.260574    1.6236869  18.465319  -16.130207 ]]\n",
      "148 -th evaluation done. score: 5605.597230468847 step: 1000\n",
      "step 1000 score 5502.386541967066 qloss 51.815556 ploss -301.30615\n",
      "ac [[-0.99954945  1.         -1.          0.98863137  1.         -1.        ]] ua [[ -4.1989727  63.539764  -25.634151    2.5821657  28.764883  -23.187984 ]]\n",
      "149 -th evaluation done. score: 5582.886554617694 step: 1000\n",
      "step 1000 score 5570.340795014642 qloss 29.061111 ploss -305.6644\n",
      "ac [[0.1165871 1.        1.        0.7399926 1.        1.       ]] ua [[ 0.1171197 21.214706  21.753956   0.950463  25.35157   19.67534  ]]\n",
      "150 -th evaluation done. score: 5719.664479158341 step: 1000\n",
      "step 1000 score 5664.195721986028 qloss 25.75712 ploss -303.48965\n",
      "ac [[-0.08695703  1.          1.          0.6484386   1.          1.        ]] ua [[-0.08717722 39.929054   37.447563    0.7725997  13.843529   12.36776   ]]\n",
      "151 -th evaluation done. score: 5685.990921888942 step: 1000\n",
      "step 1000 score 5634.658378312577 qloss 25.88431 ploss -301.89575\n",
      "ac [[0.33676776 1.         1.         0.841954   1.         1.        ]] ua [[ 0.35044235 39.905945   21.222393    1.2278482  17.148493   22.06805   ]]\n",
      "152 -th evaluation done. score: 5572.99855628289 step: 1000\n",
      "step 1000 score 5721.083174507223 qloss 24.372448 ploss -300.51852\n",
      "ac [[0.04882396 1.         1.         0.47125942 1.         1.        ]] ua [[ 0.04886282 26.514755   37.57956     0.511688   22.255098   20.705667  ]]\n",
      "153 -th evaluation done. score: 5673.344384888933 step: 1000\n",
      "step 1000 score 5629.239451474541 qloss 33.41843 ploss -311.4788\n",
      "ac [[-0.19255342  1.          1.          0.6223116   1.          1.        ]] ua [[-0.19498757 28.45945    27.34062     0.7287688  20.77221    17.393635  ]]\n",
      "154 -th evaluation done. score: 5654.081399832365 step: 1000\n",
      "step 1000 score 5669.2692646681535 qloss 26.03886 ploss -316.12814\n",
      "ac [[0.17083938 1.         1.         0.9457404  1.         1.        ]] ua [[ 0.17253116 23.746555   11.59128     1.7898084  19.772596   29.15139   ]]\n",
      "155 -th evaluation done. score: 5600.731697795173 step: 1000\n",
      "step 1000 score 5744.858903525014 qloss 22.322426 ploss -316.9171\n",
      "ac [[-1.          0.999998   -1.         -0.9998795  -1.          0.08589776]] ua [[-11.299149     6.8718276  -24.578781    -4.858328   -20.402298\n",
      "    0.08610997]]\n",
      "156 -th evaluation done. score: 5745.028534122979 step: 1000\n",
      "step 1000 score 5689.0403534015195 qloss 19.794321 ploss -319.27612\n",
      "ac [[0.12264886 1.         1.         0.7821505  1.         1.        ]] ua [[ 0.12326948 28.720093   32.137253    1.0508857  23.369804   22.658325  ]]\n",
      "157 -th evaluation done. score: 5800.975135805804 step: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000 score 5661.449206613346 qloss 39.174057 ploss -322.19992\n",
      "ac [[-0.24699092  1.          1.          0.75181794  1.          0.9999983 ]] ua [[-0.2522057 43.420746  29.219957   0.9771233 18.742828   7.009966 ]]\n",
      "158 -th evaluation done. score: 5709.227404868697 step: 1000\n",
      "step 1000 score 5576.5188283328425 qloss 20.777063 ploss -323.5782\n",
      "ac [[-0.99991727  1.         -1.          0.99828714  1.         -1.        ]] ua [[ -5.045918   66.87081   -43.496258    3.5309556  22.49062   -36.60974  ]]\n",
      "159 -th evaluation done. score: 5615.294701712621 step: 1000\n",
      "step 1000 score 5620.5370741615025 qloss 27.707294 ploss -331.8988\n",
      "ac [[-1.          1.         -1.          0.98606557  1.         -1.        ]] ua [[ -9.725149   78.55258   -40.48321     2.4797692  50.958572  -43.103546 ]]\n",
      "160 -th evaluation done. score: 5745.413939735776 step: 1000\n",
      "step 1000 score 5679.917555764362 qloss 23.370262 ploss -333.57602\n",
      "ac [[0.22020729 1.         1.         0.9748078  1.         1.        ]] ua [[ 0.22387399 31.056475   25.200527    2.180846   32.25483    47.392925  ]]\n",
      "161 -th evaluation done. score: 5735.06230194145 step: 1000\n",
      "step 1000 score 5581.4849772157295 qloss 28.887024 ploss -336.43683\n",
      "ac [[ 1.         -0.99999964  1.         -0.9999997  -1.          1.        ]] ua [[ 40.85194    -7.6452427  51.138443   -7.7854857 -32.50839    25.13942  ]]\n",
      "162 -th evaluation done. score: 5873.015335532273 step: 1000\n",
      "step 1000 score 5784.056510590088 qloss 25.053637 ploss -340.56668\n",
      "ac [[0.41018063 1.         1.         0.87753785 1.         1.        ]] ua [[ 0.43582842 39.12523    35.01782     1.3649572  23.282436   30.063168  ]]\n",
      "163 -th evaluation done. score: 5815.347776173944 step: 1000\n",
      "step 1000 score 5805.563073635382 qloss 21.273155 ploss -342.72055\n",
      "ac [[0.03220671 1.         1.         0.6943925  1.         1.        ]] ua [[3.2217853e-02 2.5936516e+01 3.6815784e+01 8.5638893e-01 2.5342852e+01\n",
      "  2.3577353e+01]]\n",
      "164 -th evaluation done. score: 5782.7919471921105 step: 1000\n",
      "step 1000 score 5689.740782265087 qloss 24.492275 ploss -344.65652\n",
      "ac [[-1.         1.        -1.         0.9923555  1.        -1.       ]] ua [[-12.334096   58.668945  -57.35985     2.7815363  34.972626  -25.056759 ]]\n",
      "165 -th evaluation done. score: 5646.78314289186 step: 1000\n",
      "step 1000 score 5713.360888459534 qloss 39.737545 ploss -341.63354\n",
      "ac [[0.04214346 1.         1.         0.6975278  1.         1.        ]] ua [[ 0.04216844 31.928167   30.207726    0.8624694  30.57899    32.989193  ]]\n",
      "166 -th evaluation done. score: 5725.503272693386 step: 1000\n",
      "step 1000 score 5756.655267540659 qloss 50.717205 ploss -349.89584\n",
      "ac [[-0.09717466  1.          1.          0.7020735   1.          1.        ]] ua [[-0.09748228 27.963797   32.329678    0.8713778  31.638355   38.423565  ]]\n",
      "167 -th evaluation done. score: 5626.256274282936 step: 1000\n",
      "step 1000 score 5768.317533388021 qloss 42.89629 ploss -350.94034\n",
      "ac [[-0.08446856  1.          1.          0.32764393  1.          1.        ]] ua [[-0.08467032 37.46922    36.854427    0.34018657 34.500954   30.076607  ]]\n",
      "168 -th evaluation done. score: 5769.169320512122 step: 1000\n",
      "step 1000 score 5746.884459545485 qloss 22.853907 ploss -356.2533\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 68.92717  -27.516762  77.81144  -13.561959 -44.57041   40.62618 ]]\n",
      "169 -th evaluation done. score: 5686.577087291077 step: 1000\n",
      "step 1000 score 5630.183071306726 qloss 14.277016 ploss -354.3538\n",
      "ac [[-1.         1.        -1.         0.9180743  1.        -1.       ]] ua [[-12.5374365  90.29114   -30.224663    1.5766321  35.20722   -27.679155 ]]\n",
      "170 -th evaluation done. score: 5621.901046450334 step: 1000\n",
      "step 1000 score 5732.7421746085465 qloss 14.751732 ploss -355.51965\n",
      "ac [[-0.9999999  1.        -1.         0.9971547  1.        -1.       ]] ua [[ -8.066598  50.64219  -51.888084   3.276918  37.023014 -21.844545]]\n",
      "171 -th evaluation done. score: 5771.457038647279 step: 1000\n",
      "step 1000 score 5735.17809374247 qloss 23.95219 ploss -358.6284\n",
      "ac [[-0.09834633  1.          1.          0.6910301   1.          1.        ]] ua [[-0.09866527 33.62712    35.38271     0.8499245  35.272625   38.063526  ]]\n",
      "172 -th evaluation done. score: 5608.60853029082 step: 1000\n",
      "step 1000 score 5753.734484625468 qloss 16.721148 ploss -365.511\n",
      "ac [[0.26335025 1.         1.         0.8923601  1.         1.        ]] ua [[ 0.26970494 38.908566   35.0562      1.433394   29.114853   28.47041   ]]\n",
      "173 -th evaluation done. score: 5821.205098299133 step: 1000\n",
      "step 1000 score 5749.2153573807755 qloss 62.068356 ploss -365.36368\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 51.37257  -26.149038  73.286285  -8.917097 -25.008303  44.389473]]\n",
      "174 -th evaluation done. score: 5740.377018783047 step: 1000\n",
      "step 1000 score 5737.134197247129 qloss 15.90839 ploss -368.28354\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 65.64412  -21.8413    73.870964 -13.640597 -35.511127  42.327988]]\n",
      "175 -th evaluation done. score: 5828.343928208588 step: 1000\n",
      "step 1000 score 5741.073904336078 qloss 15.687719 ploss -370.3162\n",
      "ac [[-1.         1.        -1.         0.8921084  1.        -1.       ]] ua [[-14.420307   82.26408   -47.64725     1.4321599  33.571278  -44.569077 ]]\n",
      "176 -th evaluation done. score: 5891.325092147677 step: 1000\n",
      "step 1000 score 5781.508108330981 qloss 26.437168 ploss -371.80014\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 58.33613  -18.865791  74.57186  -11.237855 -34.37372   36.8525  ]]\n",
      "177 -th evaluation done. score: 5962.394890592935 step: 1000\n",
      "step 1000 score 5810.585956270655 qloss 30.080233 ploss -374.96298\n",
      "ac [[0.0589603  1.         1.         0.57925874 1.         1.        ]] ua [[ 0.05902877 26.460436   25.628061    0.6613464  35.163834   32.828938  ]]\n",
      "178 -th evaluation done. score: 6025.186071702861 step: 1000\n",
      "step 1000 score 5831.3114945095795 qloss 38.475395 ploss -373.874\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 50.507133 -33.396618  79.91199  -12.497176 -25.611168  38.834183]]\n",
      "179 -th evaluation done. score: 5824.963034842808 step: 1000\n",
      "step 1000 score 5825.9344826854185 qloss 24.837252 ploss -380.66714\n",
      "ac [[-1.         1.        -1.        -0.9942014 -1.        -0.3100319]] ua [[-14.393469    15.903154   -48.454384    -2.920183   -16.46184\n",
      "   -0.32058072]]\n",
      "180 -th evaluation done. score: 5932.819209656408 step: 1000\n",
      "step 1000 score 5916.254109877359 qloss 16.674952 ploss -376.1688\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 78.91086  -23.88351   69.01437  -13.74566  -31.405485  48.244198]]\n",
      "181 -th evaluation done. score: 5545.081649725349 step: 1000\n",
      "step 1000 score 5905.068770749151 qloss 21.259083 ploss -381.84335\n",
      "ac [[-0.21014233  1.          1.          0.6329437   1.          1.        ]] ua [[-0.21332026 36.3623     31.999935    0.7463122  15.833697   29.756977  ]]\n",
      "182 -th evaluation done. score: 6095.79587996751 step: 1000\n",
      "step 1000 score 5756.833080319979 qloss 12.251373 ploss -381.1614\n",
      "ac [[0.25224203 1.         1.         0.88082105 1.         1.        ]] ua [[ 0.2578058 32.793095  42.60221    1.379419  38.03089   29.119318 ]]\n",
      "183 -th evaluation done. score: 5736.102848344172 step: 1000\n",
      "step 1000 score 5927.176055235074 qloss 15.358742 ploss -387.34106\n",
      "ac [[-0.04843475  1.          1.          0.85959935  1.          1.        ]] ua [[-4.8472684e-02  4.3333313e+01  2.8704105e+01  1.2918079e+00\n",
      "   3.6992844e+01  4.9189125e+01]]\n",
      "184 -th evaluation done. score: 5974.850238297841 step: 1000\n",
      "step 1000 score 5919.3677248941 qloss 14.782941 ploss -391.8179\n",
      "ac [[0.02245249 1.         1.         0.5905033  1.         1.        ]] ua [[2.2456272e-02 3.0620148e+01 2.6550072e+01 6.7843843e-01 3.2726063e+01\n",
      "  3.4629570e+01]]\n",
      "185 -th evaluation done. score: 5843.007669210952 step: 1000\n",
      "step 1000 score 5903.619615828465 qloss 15.37397 ploss -385.8815\n",
      "ac [[0.1319502 1.        1.        0.640834  1.        1.       ]] ua [[ 0.1327241  35.12216    37.91878     0.75958747 38.164944    9.824276  ]]\n",
      "186 -th evaluation done. score: 5927.710631758208 step: 1000\n",
      "step 1000 score 5962.091582549523 qloss 17.305761 ploss -390.3452\n",
      "ac [[-1.         1.        -1.         0.8151018  1.        -1.       ]] ua [[-35.013256   74.2435    -62.390915    1.1420456  40.687157  -32.011833 ]]\n",
      "187 -th evaluation done. score: 5888.167479718897 step: 1000\n",
      "step 1000 score 5847.018777227492 qloss 10.634743 ploss -397.50668\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 74.40477  -44.534573  89.74729  -17.073177 -53.062973  48.752914]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 -th evaluation done. score: 5862.547554794543 step: 1000\n",
      "step 1000 score 5915.642887777462 qloss 17.422043 ploss -394.34708\n",
      "ac [[ 1.        -1.         1.        -0.9999485 -1.         1.       ]] ua [[ 54.543373 -27.31657   64.93784   -5.28382  -31.261894  39.388313]]\n",
      "189 -th evaluation done. score: 6088.163840464136 step: 1000\n",
      "step 1000 score 5960.249169231494 qloss 14.031288 ploss -398.82462\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 61.33621  -29.00826   70.44394  -13.019157 -49.94406   47.26189 ]]\n",
      "190 -th evaluation done. score: 5936.924607789759 step: 1000\n",
      "step 1000 score 5909.71070757868 qloss 19.600864 ploss -401.86758\n",
      "ac [[0.05725919 1.         1.         0.9634307  1.         1.        ]] ua [[ 0.0573219 43.2853    38.59253    1.9916192 45.669186  49.012238 ]]\n",
      "191 -th evaluation done. score: 6067.237918332394 step: 1000\n",
      "step 1000 score 5885.42198783804 qloss 17.341263 ploss -403.5175\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 43.598816 -19.964598  59.06859  -13.960747 -34.058064  32.258198]]\n",
      "192 -th evaluation done. score: 5945.02601006386 step: 1000\n",
      "step 1000 score 6034.58952117177 qloss 19.113396 ploss -401.52905\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 63.223465 -33.96677   70.75898  -17.111792 -49.14386   49.20676 ]]\n",
      "193 -th evaluation done. score: 6180.74283985853 step: 1000\n",
      "step 1000 score 6033.923523830461 qloss 14.340445 ploss -407.536\n",
      "ac [[0.01662806 1.         1.         0.76736253 1.         1.        ]] ua [[1.6629595e-02 2.1627296e+01 2.4290056e+01 1.0138810e+00 3.5222095e+01\n",
      "  3.4813229e+01]]\n",
      "194 -th evaluation done. score: 6075.007653970098 step: 1000\n",
      "step 1000 score 5958.567990754481 qloss 17.561764 ploss -410.28943\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 62.858646 -46.395138  79.953094 -31.820051 -40.067753  39.350002]]\n",
      "195 -th evaluation done. score: 5868.316201705241 step: 1000\n",
      "step 1000 score 5952.362619309882 qloss 15.688499 ploss -408.61548\n",
      "ac [[-1.          1.         -1.          0.79949623  1.         -1.        ]] ua [[-31.801432   79.85404   -55.05657     1.0972143  52.57378   -43.072247 ]]\n",
      "196 -th evaluation done. score: 5900.498717074884 step: 1000\n",
      "step 1000 score 5856.1983938421345 qloss 12.8870945 ploss -412.14218\n",
      "ac [[0.3636291  1.         1.         0.64931315 1.         1.        ]] ua [[ 0.38106164 41.94532    42.39676     0.7741102  33.64902    29.964363  ]]\n",
      "197 -th evaluation done. score: 6170.209748467615 step: 1000\n",
      "step 1000 score 5992.942516859008 qloss 22.72827 ploss -412.83725\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 57.82617  -35.219227  72.69069  -23.36094  -47.257366  37.7145  ]]\n",
      "198 -th evaluation done. score: 5967.472223913679 step: 1000\n",
      "step 1000 score 6061.757900770057 qloss 26.2481 ploss -415.79434\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 72.304665 -34.821587  79.66912  -28.084208 -51.459507  48.15278 ]]\n",
      "199 -th evaluation done. score: 6003.4680619337305 step: 1000\n",
      "step 1000 score 5973.477907688757 qloss 9.256641 ploss -416.55078\n",
      "ac [[-1.         1.        -1.        -0.9999989 -1.        -0.9999386]] ua [[-44.828114   47.71488   -72.26798    -7.2219033 -48.245747   -5.1960716]]\n",
      "200 -th evaluation done. score: 6028.659581654105 step: 1000\n",
      "step 1000 score 5990.581681307696 qloss 16.399832 ploss -417.36563\n",
      "ac [[0.98135144 1.         1.         0.97698087 1.         1.        ]] ua [[ 2.332879 26.636997 32.404984  2.2265   39.092663 47.200577]]\n",
      "201 -th evaluation done. score: 5923.528858849099 step: 1000\n",
      "step 1000 score 5876.581086504638 qloss 14.565739 ploss -418.73618\n",
      "ac [[-1.         1.        -1.         0.9993093  1.        -1.       ]] ua [[-40.039368  76.34872  -67.50147    3.985355  54.499992 -47.848747]]\n",
      "202 -th evaluation done. score: 6112.514769514577 step: 1000\n",
      "step 1000 score 6119.712474073519 qloss 17.656637 ploss -420.98526\n",
      "ac [[0.11979672 1.         1.         0.93480605 1.         1.        ]] ua [[ 0.12037479 46.42227    38.91246     1.6951984  42.932888   39.111206  ]]\n",
      "203 -th evaluation done. score: 6085.125414234615 step: 1000\n",
      "step 1000 score 5983.899177522809 qloss 15.661015 ploss -419.5057\n",
      "ac [[-1.         1.        -1.         0.9882991  1.        -1.       ]] ua [[-28.64192    91.8494    -44.22081     2.5676801  43.8425    -31.129808 ]]\n",
      "204 -th evaluation done. score: 5998.834655769506 step: 1000\n",
      "step 1000 score 6022.10845318156 qloss 10.605554 ploss -424.05923\n",
      "ac [[-1.          1.         -1.          0.99285823  1.         -1.        ]] ua [[-34.905136   93.117424  -62.450012    2.8156755  49.985146  -37.445683 ]]\n",
      "205 -th evaluation done. score: 6239.25023252943 step: 1000\n",
      "step 1000 score 6053.987969694037 qloss 15.619078 ploss -424.53897\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 70.339615 -45.17773   74.80775  -32.888977 -54.768913  61.71078 ]]\n",
      "206 -th evaluation done. score: 5982.8431383137795 step: 1000\n",
      "step 1000 score 6040.837039528948 qloss 18.87946 ploss -428.10254\n",
      "ac [[0.37813452 1.         1.         0.9907507  1.         1.        ]] ua [[ 0.39788115 39.522476   35.11331     2.6858556  40.53271    46.786915  ]]\n",
      "207 -th evaluation done. score: 6018.666500236843 step: 1000\n",
      "step 1000 score 5842.974908153927 qloss 11.560217 ploss -432.46494\n",
      "ac [[-1.         1.        -1.         0.9289042  1.        -1.       ]] ua [[-43.38226   124.345146  -56.09706     1.6503392  58.396767  -41.519775 ]]\n",
      "208 -th evaluation done. score: 6127.574333202602 step: 1000\n",
      "step 1000 score 6094.108351793472 qloss 9.402489 ploss -433.3246\n",
      "ac [[-1.         1.        -1.         0.9354056  1.        -1.       ]] ua [[-36.29439    93.86833   -55.990025    1.6999727  51.504192  -51.990765 ]]\n",
      "209 -th evaluation done. score: 6114.459856703817 step: 1000\n",
      "step 1000 score 6005.910208486578 qloss 7.641389 ploss -433.9163\n",
      "ac [[0.86953217 1.         1.         0.9758676  1.         1.        ]] ua [[ 1.3311582 45.211216  29.342674   2.2026048 36.86035   27.184807 ]]\n",
      "210 -th evaluation done. score: 5812.420286262639 step: 1000\n",
      "step 1000 score 6025.4829308798535 qloss 7.204648 ploss -436.52872\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[ 63.97217  -35.363102  77.58085  -44.269115 -51.045414  54.336834]]\n",
      "211 -th evaluation done. score: 5959.380810822149 step: 1000\n",
      "step 1000 score 6039.561259409381 qloss 4.866361 ploss -437.70605\n",
      "ac [[0.9957915 1.        1.        0.9968997 1.        1.       ]] ua [[ 3.0808496 59.175446  51.95172    3.2339349 33.605896  24.946018 ]]\n",
      "212 -th evaluation done. score: 5952.772480833011 step: 1000\n",
      "step 1000 score 5927.446811041154 qloss 20.113161 ploss -436.44824\n",
      "ac [[0.05665255 1.         1.         0.9225409  1.         1.        ]] ua [[ 0.05671329 30.556065   39.202904    1.6058273  41.117455   28.58574   ]]\n",
      "213 -th evaluation done. score: 6205.425797306886 step: 1000\n",
      "step 1000 score 5986.707926010094 qloss 10.026299 ploss -440.5857\n",
      "ac [[-1.         1.        -1.         0.9994269  1.        -1.       ]] ua [[-23.595392  108.76026   -55.532116    4.0787606  47.14013   -46.656208 ]]\n",
      "214 -th evaluation done. score: 6226.6402254617815 step: 1000\n",
      "step 1000 score 6117.6587098339005 qloss 7.1253676 ploss -437.25235\n",
      "ac [[0.38029033 1.         1.         0.76846135 1.         1.        ]] ua [[ 0.40039903 36.776302   39.065224    1.0165591  25.009583   22.198887  ]]\n",
      "215 -th evaluation done. score: 6200.485985102958 step: 1000\n",
      "step 1000 score 6010.291191987525 qloss 7.5182953 ploss -442.57333\n",
      "ac [[0.01915704 1.         1.         0.743693   1.         1.        ]] ua [[1.9159390e-02 4.6828381e+01 3.8967106e+01 9.5869219e-01 2.9494822e+01\n",
      "  4.0254261e+01]]\n",
      "216 -th evaluation done. score: 6065.982313119261 step: 1000\n",
      "step 1000 score 5982.948045791052 qloss 8.2300825 ploss -445.8755\n",
      "ac [[-1.         1.        -1.         0.8727713  1.        -1.       ]] ua [[-37.401474   82.66841   -43.297356    1.3445941  42.522457  -48.24662  ]]\n",
      "217 -th evaluation done. score: 6264.99731359761 step: 1000\n",
      "step 1000 score 6161.935778555554 qloss 6.090332 ploss -448.16846\n",
      "ac [[-0.34864974  1.          1.          0.799536    1.          1.        ]] ua [[-0.36390585 41.515366   32.697083    1.0973247  35.922615   39.838276  ]]\n",
      "218 -th evaluation done. score: 6160.596217872816 step: 1000\n",
      "step 1000 score 6188.810469341718 qloss 9.665581 ploss -448.85635\n",
      "ac [[ 0.39018527  1.          1.         -0.03295507  1.          0.9759301 ]] ua [[ 4.1201860e-01  2.3944048e+01  4.3446861e+01 -3.2967012e-02\n",
      "   1.4166626e+01  2.2039165e+00]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 -th evaluation done. score: 6207.3958293443075 step: 1000\n",
      "step 1000 score 6117.656220045659 qloss 9.09502 ploss -447.2992\n",
      "ac [[0.49736768 1.         1.         0.9495574  1.         1.        ]] ua [[ 0.54580253 33.258705   10.731781    1.8272613  38.311295   41.262638  ]]\n",
      "220 -th evaluation done. score: 6110.532843724153 step: 1000\n",
      "step 1000 score 6038.423225933092 qloss 9.153085 ploss -445.34546\n",
      "ac [[0.6866006 1.        1.        0.9964045 1.        1.       ]] ua [[ 0.841496  47.838287  51.35971    3.1597342 37.900116  50.065033 ]]\n",
      "221 -th evaluation done. score: 6157.494927306525 step: 1000\n",
      "step 1000 score 6211.05751362044 qloss 13.430384 ploss -451.5663\n",
      "ac [[-0.16101211  1.          1.          0.7207117   1.          1.        ]] ua [[-0.16242559 38.947685   50.556248    0.9091242  37.126083   35.314175  ]]\n",
      "222 -th evaluation done. score: 6283.1833000568995 step: 1000\n",
      "step 1000 score 6149.338923800023 qloss 13.745831 ploss -452.93057\n",
      "ac [[-1.          1.         -1.          0.99823564  1.         -1.        ]] ua [[-30.478945  103.78489   -42.60737     3.5161052  60.796085  -44.437744 ]]\n",
      "223 -th evaluation done. score: 6119.592803017607 step: 1000\n",
      "step 1000 score 6066.476648749096 qloss 9.981101 ploss -453.49707\n",
      "ac [[-1.         1.        -1.         0.8547779  1.        -1.       ]] ua [[-27.21378    95.536026  -54.340298    1.2736278  41.033672  -54.10283  ]]\n",
      "224 -th evaluation done. score: 6106.867014073616 step: 1000\n",
      "step 1000 score 6097.108240650316 qloss 11.42494 ploss -456.4396\n",
      "ac [[0.10175642 1.         1.         0.6273264  1.         1.        ]] ua [[ 0.10210983 34.529976   45.315613    0.73699546 24.360985   20.394169  ]]\n",
      "225 -th evaluation done. score: 6209.320984280204 step: 1000\n",
      "step 1000 score 6185.165056798523 qloss 14.395453 ploss -452.84634\n",
      "ac [[-1.         1.        -1.         0.9999802  1.        -1.       ]] ua [[-32.52043   106.649185  -60.313263    5.7611637  64.096596  -49.774086 ]]\n",
      "226 -th evaluation done. score: 6094.438690038396 step: 1000\n",
      "step 1000 score 6065.550287338515 qloss 7.4681187 ploss -458.65247\n",
      "ac [[-0.02694177  1.          1.          0.6589562   1.          1.        ]] ua [[-2.6948296e-02  3.9953213e+01  3.9884159e+01  7.9096645e-01\n",
      "   3.6336464e+01  3.8734337e+01]]\n",
      "227 -th evaluation done. score: 6166.159421719277 step: 1000\n",
      "step 1000 score 6060.263329816133 qloss 8.52496 ploss -461.827\n",
      "ac [[-1.          1.         -1.          0.94968635  1.         -1.        ]] ua [[-36.593662  105.649826  -42.127357    1.8285738  61.62597   -47.29415  ]]\n",
      "228 -th evaluation done. score: 6217.014026803322 step: 1000\n",
      "step 1000 score 6228.07614726751 qloss 10.00034 ploss -459.48862\n",
      "ac [[-0.06333657  1.          1.          0.82440096  1.          1.        ]] ua [[-0.06342147 48.768955   34.53491     1.1704018  43.399193   43.866203  ]]\n",
      "229 -th evaluation done. score: 6296.960309784167 step: 1000\n",
      "step 1000 score 5974.124281223129 qloss 15.297823 ploss -463.1122\n",
      "ac [[-0.3051502  1.         1.         0.6426816  1.         1.       ]] ua [[-0.31518888 37.78324    34.031162    0.762729   35.740818   25.583801  ]]\n",
      "230 -th evaluation done. score: 6317.883129106364 step: 1000\n",
      "step 1000 score 5973.074390792045 qloss 10.382067 ploss -461.87326\n",
      "ac [[0.09983456 1.         1.         0.53312767 1.         1.        ]] ua [[ 0.10016825 44.647915   40.29098     0.5945046  29.582289   23.886248  ]]\n",
      "231 -th evaluation done. score: 6219.894906828484 step: 1000\n",
      "step 1000 score 6199.9148223320635 qloss 14.124916 ploss -462.14664\n",
      "ac [[-1.          1.         -1.          0.98615766  1.         -1.        ]] ua [[-51.854176  107.34375   -53.66607     2.4831092  62.770115  -43.330975 ]]\n",
      "232 -th evaluation done. score: 6240.83010357213 step: 1000\n",
      "step 1000 score 6075.069953961623 qloss 8.377178 ploss -465.116\n",
      "ac [[-1.        1.       -1.        0.935515  1.       -1.      ]] ua [[-40.375916  117.69928   -46.637768    1.7008481  62.677605  -62.26511  ]]\n",
      "233 -th evaluation done. score: 6216.105015159993 step: 1000\n",
      "step 1000 score 6080.470466085798 qloss 8.048109 ploss -466.1735\n",
      "ac [[0.2111904 1.        1.        0.6253078 1.        1.       ]] ua [[ 0.21441701 44.28531    48.337406    0.73367375 48.386738   29.991669  ]]\n",
      "234 -th evaluation done. score: 6166.01154745205 step: 1000\n",
      "step 1000 score 6132.239223172149 qloss 9.655586 ploss -463.13818\n",
      "ac [[0.22389354 1.         1.         0.8475561  1.         1.        ]] ua [[ 0.22775142 29.896303   35.55395     1.2474113  31.60646    47.88101   ]]\n",
      "235 -th evaluation done. score: 6296.375477873846 step: 1000\n",
      "step 1000 score 6212.331473089467 qloss 9.868173 ploss -470.75406\n",
      "ac [[-1.          1.         -1.          0.92175674  1.         -1.        ]] ua [[-38.362648   81.98085   -52.730865    1.6005863  41.170967  -38.020653 ]]\n",
      "236 -th evaluation done. score: 6061.0929433586925 step: 1000\n",
      "step 1000 score 6105.774631186765 qloss 21.551796 ploss -471.313\n",
      "ac [[-0.18730265  1.          1.          0.67667675  1.          1.        ]] ua [[-0.18954028 45.936375   36.40758     0.82295805 42.08551    47.724155  ]]\n",
      "237 -th evaluation done. score: 6197.633975591676 step: 1000\n",
      "step 1000 score 6319.613598358181 qloss 10.365403 ploss -470.0471\n",
      "ac [[0.01145245 1.         1.         0.54096615 1.         1.        ]] ua [[1.1452951e-02 3.8305050e+01 3.9432610e+01 6.0552049e-01 4.0583221e+01\n",
      "  3.7854477e+01]]\n",
      "238 -th evaluation done. score: 6199.81944264827 step: 1000\n",
      "step 1000 score 6276.157200907273 qloss 13.800962 ploss -469.27017\n",
      "ac [[-0.1436783   1.          1.          0.79419017  1.          1.        ]] ua [[-0.14467941 42.938576   39.269104    1.0826782  24.61938    52.846386  ]]\n",
      "239 -th evaluation done. score: 6140.25939320435 step: 1000\n",
      "step 1000 score 6223.607259873436 qloss 6.188631 ploss -472.72745\n",
      "ac [[-0.23712029  1.          1.          0.67418534  1.          1.        ]] ua [[-0.24172063 48.758972   35.15233     0.8183766  28.152813   22.151897  ]]\n",
      "240 -th evaluation done. score: 6047.712790159119 step: 1000\n",
      "step 1000 score 6264.851227644087 qloss 8.009653 ploss -473.97183\n",
      "ac [[0.04553673 1.         1.         0.9830086  1.         1.        ]] ua [[4.5568250e-02 4.3570663e+01 3.7034367e+01 2.3798344e+00 3.0987669e+01\n",
      "  4.5872055e+01]]\n",
      "241 -th evaluation done. score: 6165.53192134282 step: 1000\n",
      "step 1000 score 6208.419895729111 qloss 17.29166 ploss -475.0735\n",
      "ac [[-0.04399114  1.          1.          0.8467087   1.          1.        ]] ua [[-4.4019550e-02  3.9355358e+01  2.8052164e+01  1.2444099e+00\n",
      "   4.4743168e+01  3.6538582e+01]]\n",
      "242 -th evaluation done. score: 6145.667376447063 step: 1000\n",
      "step 1000 score 6214.029676048083 qloss 6.4997635 ploss -481.18002\n",
      "ac [[0.11718422 1.         1.         0.99855375 1.         1.        ]] ua [[ 0.11772508 32.35835    40.057114    3.615604   42.92075    46.23947   ]]\n",
      "243 -th evaluation done. score: 6070.2056692317055 step: 1000\n",
      "step 1000 score 6164.545520507097 qloss 17.355244 ploss -479.303\n",
      "ac [[0.23328218 1.         1.         0.9617117  1.         1.        ]] ua [[ 0.23765779 38.763      20.05647     1.9682146  46.515415   33.109962  ]]\n",
      "244 -th evaluation done. score: 6282.840050025465 step: 1000\n",
      "step 1000 score 6194.007890648613 qloss 6.3807163 ploss -477.9838\n",
      "ac [[0.22247627 1.         0.99999535 0.9705266  1.         1.        ]] ua [[ 0.22625986 44.5955      6.4836254   2.101283   39.30469    40.304905  ]]\n",
      "245 -th evaluation done. score: 6228.042912615285 step: 1000\n",
      "step 1000 score 6112.873285235176 qloss 10.994448 ploss -481.4453\n",
      "ac [[0.72207606 1.         1.         0.99863887 1.         1.        ]] ua [[ 0.91196907 44.35767    43.087234    3.6459293  34.439255   31.571676  ]]\n",
      "246 -th evaluation done. score: 6374.173344861814 step: 1000\n",
      "step 1000 score 6114.260079628989 qloss 7.8503385 ploss -480.50058\n",
      "ac [[0.5364477 1.        1.        0.9959336 1.        1.       ]] ua [[ 0.5991546 50.840584  22.994738   3.0980432 39.841457  42.48763  ]]\n",
      "247 -th evaluation done. score: 6234.652730586851 step: 1000\n",
      "step 1000 score 6095.124403634456 qloss 7.6268053 ploss -483.41458\n",
      "ac [[0.27169746 1.         1.         0.9746154  1.         1.        ]] ua [[ 0.2786957 41.063797  35.40443    2.1769934 48.387527  36.202248 ]]\n",
      "248 -th evaluation done. score: 6223.916184736746 step: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000 score 6172.049709747687 qloss 12.608957 ploss -484.1483\n",
      "ac [[0.10460867 1.         1.         0.97242975 1.         1.        ]] ua [[ 0.10499278 44.73298    29.04691     2.1351433  32.86862    45.933342  ]]\n",
      "249 -th evaluation done. score: 6287.306667721186 step: 1000\n",
      "step 1000 score 6231.315842844225 qloss 12.482275 ploss -485.45334\n",
      "ac [[0.07176062 1.         1.         0.9530569  1.         1.        ]] ua [[ 0.07188419 17.027199   34.753536    1.8641068  22.993124   33.77913   ]]\n",
      "250 -th evaluation done. score: 6058.878988441875 step: 1000\n",
      "step 1000 score 6286.352538402365 qloss 11.738058 ploss -487.77905\n",
      "ac [[-0.0057322   1.          1.          0.81361663  1.          1.        ]] ua [[-5.7322597e-03  4.8571583e+01  3.7946705e+01  1.1376363e+00\n",
      "   2.5406830e+01  5.2753178e+01]]\n",
      "251 -th evaluation done. score: 6187.957536750979 step: 1000\n",
      "step 1000 score 6281.703690943987 qloss 11.538975 ploss -488.59286\n",
      "ac [[-0.20309193  1.          1.          0.8053965   1.          1.        ]] ua [[-0.20595543 30.878529   43.405552    1.1137855  38.14846    34.83916   ]]\n",
      "252 -th evaluation done. score: 6129.410573337878 step: 1000\n",
      "step 1000 score 6191.036389388758 qloss 11.972734 ploss -489.89987\n",
      "ac [[-1.        1.       -1.        0.935924  1.       -1.      ]] ua [[-57.176235  104.716705  -53.545464    1.7041358  58.900448  -40.725647 ]]\n",
      "253 -th evaluation done. score: 6346.847657347009 step: 1000\n",
      "step 1000 score 6153.851597576049 qloss 6.6014533 ploss -491.52023\n",
      "ac [[0.99700034 1.         1.         0.05917342 0.9999998  1.        ]] ua [[ 3.2504733  30.603432   21.424751    0.05924263  8.385578   27.102528  ]]\n",
      "254 -th evaluation done. score: 6289.440182111194 step: 1000\n",
      "step 1000 score 6188.2576693863175 qloss 6.0470705 ploss -493.1259\n",
      "ac [[-0.16689357  1.          1.          0.78038865  1.          1.        ]] ua [[-0.16846952 41.837894   33.394306    1.0463637  32.822094   43.373787  ]]\n",
      "255 -th evaluation done. score: 6304.597529816421 step: 1000\n",
      "step 1000 score 6190.759069610579 qloss 6.4240313 ploss -494.14893\n",
      "ac [[-0.02087552  1.          1.          0.71738905  1.          1.        ]] ua [[-2.0878559e-02  4.0960262e+01  3.5910255e+01  9.0224451e-01\n",
      "   3.2954765e+01  3.6100716e+01]]\n",
      "256 -th evaluation done. score: 6227.040254370823 step: 1000\n",
      "step 1000 score 6279.113698489886 qloss 7.012521 ploss -494.03848\n",
      "ac [[-0.4471261  1.         1.         0.6712742  1.         1.       ]] ua [[-0.48110247 34.38585    43.69005     0.81305873 42.87656    51.805614  ]]\n",
      "257 -th evaluation done. score: 6127.09279319951 step: 1000\n",
      "step 1000 score 6175.984084467004 qloss 10.148519 ploss -495.68774\n",
      "ac [[0.30093247 1.         1.         0.9948111  1.         1.        ]] ua [[ 0.31054464 33.791714   24.37055     2.9759023  38.11667    59.40501   ]]\n",
      "258 -th evaluation done. score: 6212.355501971214 step: 1000\n",
      "step 1000 score 6289.841360752153 qloss 9.363283 ploss -495.00842\n",
      "ac [[-0.15677862  1.          1.          0.69564617  1.          1.        ]] ua [[-0.15808243 35.02377    33.781693    0.8588142  37.98416    31.678928  ]]\n",
      "259 -th evaluation done. score: 6428.23119024263 step: 1000\n",
      "step 1000 score 6092.447686568054 qloss 10.813325 ploss -497.82178\n",
      "ac [[-0.17467767  1.          1.          0.6509743   1.          1.        ]] ua [[-0.17648754 47.821335   33.360413    0.7769875  29.218056   59.726448  ]]\n",
      "260 -th evaluation done. score: 6304.487230493305 step: 1000\n",
      "step 1000 score 2750.9785703607104 qloss 18.633934 ploss -499.6105\n",
      "ac [[-0.99999845  1.         -0.80592895  1.          0.9999993   1.        ]] ua [[-7.015711  31.016415  -1.1153028 10.812185   7.380855   8.684713 ]]\n",
      "261 -th evaluation done. score: 6075.861433671259 step: 1000\n",
      "step 1000 score 6140.385885559023 qloss 9.611129 ploss -498.92807\n",
      "ac [[-1.         1.        -1.         0.9758825  1.        -1.       ]] ua [[-42.16694   138.50108   -33.69345     2.2029178  59.523113  -49.6438   ]]\n",
      "262 -th evaluation done. score: 6198.810749794978 step: 1000\n",
      "step 1000 score 6350.4013701668855 qloss 16.8054 ploss -498.28897\n",
      "ac [[0.08506686 1.         1.         0.9708622  1.         1.        ]] ua [[ 0.08527295 40.892597   35.062263    2.1070948  42.507954   38.17033   ]]\n",
      "263 -th evaluation done. score: 6149.503257151966 step: 1000\n",
      "step 1000 score 6349.244468452557 qloss 16.959505 ploss -499.71085\n",
      "ac [[0.12647104 1.         1.         0.96339774 1.         1.        ]] ua [[ 0.1271519 33.247765  29.06151    1.9911617 35.216663  39.267067 ]]\n",
      "264 -th evaluation done. score: 6304.892918794355 step: 1000\n",
      "step 1000 score 6301.1849805996435 qloss 16.049753 ploss -504.75516\n",
      "ac [[0.21707022 1.         1.         0.90398186 1.         1.        ]] ua [[ 0.22057942 39.769024   46.740265    1.493583   39.951035   33.83428   ]]\n",
      "265 -th evaluation done. score: 6248.730619433198 step: 1000\n",
      "step 1000 score 6287.205000962783 qloss 8.567756 ploss -501.8248\n",
      "ac [[0.09537181 1.         1.         0.9342749  1.         1.        ]] ua [[ 0.09566256 44.966354   33.49024     1.6910037  37.80071    37.7236    ]]\n",
      "266 -th evaluation done. score: 6288.835271597006 step: 1000\n",
      "step 1000 score 6084.622289718997 qloss 15.99686 ploss -505.3683\n",
      "ac [[-0.17605463  1.          1.          0.75098306  1.          1.        ]] ua [[-0.1779082 43.765274  42.09609    0.9752058 33.443398  38.28614  ]]\n",
      "267 -th evaluation done. score: 6486.881754957869 step: 1000\n",
      "step 1000 score 6044.799403730343 qloss 21.96609 ploss -506.3734\n",
      "ac [[0.4663536 1.        1.        0.9956418 1.        1.       ]] ua [[ 0.5054003 43.74649   31.931053   3.063336  49.45564   38.692562 ]]\n",
      "268 -th evaluation done. score: 6320.989988251869 step: 1000\n",
      "step 1000 score 6320.113512254369 qloss 10.8555565 ploss -506.89404\n",
      "ac [[-1.         1.        -1.         0.9999513  1.        -1.       ]] ua [[-72.09956  125.33858  -42.60756    5.311338  71.916245 -48.880882]]\n",
      "269 -th evaluation done. score: 6314.168409247998 step: 1000\n",
      "step 1000 score 6227.9497137440185 qloss 15.228299 ploss -504.1057\n",
      "ac [[0.0762053 1.        1.        0.9237996 1.        1.       ]] ua [[ 0.07635333 45.915646   24.31938     1.6143452  35.25404    34.63717   ]]\n",
      "270 -th evaluation done. score: 6151.515731730264 step: 1000\n",
      "step 1000 score 6261.216059820605 qloss 15.691095 ploss -501.68082\n",
      "ac [[-0.07572025  1.          1.          0.7143041   1.          1.        ]] ua [[-0.07586547 38.320904   24.857721    0.8959172  29.744347   39.09408   ]]\n",
      "271 -th evaluation done. score: 6297.5630051259295 step: 1000\n",
      "step 1000 score 6263.350140703151 qloss 14.894432 ploss -512.94257\n",
      "ac [[-0.08944654  1.          1.          0.6205634   1.          1.        ]] ua [[-0.08968624 32.37823    34.60398     0.7259207  27.880886   33.68898   ]]\n",
      "272 -th evaluation done. score: 6270.172986541679 step: 1000\n",
      "step 1000 score 5609.230148703407 qloss 6.9409113 ploss -511.28067\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[104.122734 -72.28288  109.01344  -33.73455  -74.03227   79.59627 ]]\n",
      "273 -th evaluation done. score: 6184.701243327808 step: 1000\n",
      "step 1000 score 6158.160947786793 qloss 12.130042 ploss -510.88467\n",
      "ac [[-1.  1. -1.  1.  1. -1.]] ua [[-52.005356 117.91734  -58.61196   10.103531  58.948387 -65.64098 ]]\n",
      "274 -th evaluation done. score: 6329.053569030853 step: 1000\n",
      "step 1000 score 6126.326145147976 qloss 15.53599 ploss -514.9726\n",
      "ac [[0.87750286 1.         1.         0.06355349 0.83275557 1.        ]] ua [[ 1.3648051  35.368      46.838425    0.06363927  1.1970595  19.497387  ]]\n",
      "275 -th evaluation done. score: 5931.654410585181 step: 1000\n",
      "step 1000 score 6195.773300906848 qloss 7.23591 ploss -514.3687\n",
      "ac [[0.36615643 1.         1.         0.9732412  1.         1.        ]] ua [[ 0.38397717 41.29347    18.82389     2.150285   50.889786   23.568848  ]]\n",
      "276 -th evaluation done. score: 6298.120024619484 step: 1000\n",
      "step 1000 score 6054.323444119419 qloss 15.947519 ploss -510.91568\n",
      "ac [[0.337252   1.         1.         0.99883723 1.         1.        ]] ua [[ 0.35098863 50.578552   26.628319    3.724817   32.49318    46.86553   ]]\n",
      "277 -th evaluation done. score: 6275.675338219638 step: 1000\n",
      "step 1000 score 6151.6096074204 qloss 7.5160465 ploss -517.5844\n",
      "ac [[-1.  1. -1.  1.  1. -1.]] ua [[-43.40442  127.12256  -58.421886  14.679571  51.501213 -66.74542 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278 -th evaluation done. score: 6202.701555465214 step: 1000\n",
      "step 1000 score 6279.7394267466325 qloss 13.397144 ploss -514.52313\n",
      "ac [[0.1124252  1.         1.         0.69154775 1.         1.        ]] ua [[ 0.11290248 41.50089    34.510876    0.8509161  36.066566   34.413452  ]]\n",
      "279 -th evaluation done. score: 6303.716009930278 step: 1000\n",
      "step 1000 score 6210.4775917925745 qloss 20.111853 ploss -518.19244\n",
      "ac [[-0.12417394  1.          1.          0.6336964   1.          1.        ]] ua [[-0.12481814 25.849653   29.049324    0.74756896 39.21616    36.44555   ]]\n",
      "280 -th evaluation done. score: 6314.426769423549 step: 1000\n",
      "step 1000 score 6048.111126033754 qloss 9.072347 ploss -518.9043\n",
      "ac [[0.24184847 1.         1.         0.89972264 1.         1.        ]] ua [[ 0.2467365 29.022776  47.390343   1.4707612 35.79745   42.287014 ]]\n",
      "281 -th evaluation done. score: 5985.449402580576 step: 1000\n",
      "step 1000 score 6241.094542316077 qloss 15.273441 ploss -518.52484\n",
      "ac [[-0.27557647  1.          1.          0.5199842   1.          1.        ]] ua [[-0.28288865 26.085266   29.716574    0.5763181  38.952248   39.22527   ]]\n",
      "282 -th evaluation done. score: 6389.672865707286 step: 1000\n",
      "step 1000 score 6359.538493423444 qloss 12.447881 ploss -518.1362\n",
      "ac [[0.23641638 1.         1.         0.9963819  1.         1.        ]] ua [[ 0.24097493 41.221592   46.72531     3.1565762  45.573532   52.09137   ]]\n",
      "283 -th evaluation done. score: 6240.584601080581 step: 1000\n",
      "step 1000 score 6174.268719149753 qloss 14.666458 ploss -519.258\n",
      "ac [[-0.8775761   0.98787516  1.         -0.10637342  1.          1.        ]] ua [[-1.365124    2.54978    21.014162   -0.10677738 19.918383   35.426926  ]]\n",
      "284 -th evaluation done. score: 6313.046093884244 step: 1000\n",
      "step 1000 score 6259.223761075463 qloss 12.277172 ploss -520.17535\n",
      "ac [[-0.25201306  1.          1.          0.82301533  1.          1.        ]] ua [[-0.25756127 36.252155   21.381552    1.1660922  33.703106   31.297535  ]]\n",
      "285 -th evaluation done. score: 6139.720389119156 step: 1000\n",
      "step 1000 score 6152.995633589705 qloss 17.677425 ploss -523.1848\n",
      "ac [[0.27135873 1.         1.         0.9894796  1.         1.        ]] ua [[ 0.27833   27.990082  24.194002   2.6211514 36.64253   43.09998  ]]\n",
      "286 -th evaluation done. score: 6188.935545674152 step: 1000\n",
      "step 1000 score 6294.663872202361 qloss 16.208496 ploss -518.8833\n",
      "ac [[0.19888465 1.         1.         0.98681444 1.         1.        ]] ua [[ 0.20157103 37.652496   38.787876    2.5075846  38.408195   33.12257   ]]\n",
      "287 -th evaluation done. score: 6293.591668175685 step: 1000\n",
      "step 1000 score 6197.285956117385 qloss 14.144572 ploss -516.0078\n",
      "ac [[0.11335573 1.         1.         0.98135555 1.         1.        ]] ua [[ 0.11384504 39.32988    41.352543    2.3329933  47.850807   35.58239   ]]\n",
      "288 -th evaluation done. score: 6311.389859388515 step: 1000\n",
      "step 1000 score 6262.3385723002275 qloss 20.087456 ploss -524.9275\n",
      "ac [[0.24370198 1.         1.         0.99683446 1.         1.        ]] ua [[ 0.2487061 34.98993   23.460693   3.2234895 48.466515  36.537422 ]]\n",
      "289 -th evaluation done. score: 6238.33481387787 step: 1000\n",
      "step 1000 score 6326.193681352014 qloss 18.945396 ploss -521.79\n",
      "ac [[-0.21835496  1.          1.          0.45705444  1.          1.        ]] ua [[-0.22192807 34.84246    32.25034     0.4935816  41.934834   38.291973  ]]\n",
      "290 -th evaluation done. score: 6256.691949303803 step: 1000\n",
      "step 1000 score 6210.460926285003 qloss 15.277313 ploss -528.31134\n",
      "ac [[-1.          1.         -0.99999964 -1.          1.          1.        ]] ua [[-46.877678   33.91336    -7.6515083 -31.643894   53.809956    9.0833435]]\n",
      "291 -th evaluation done. score: 6291.955037255902 step: 1000\n",
      "step 1000 score 6151.4121946525565 qloss 5.592181 ploss -529.4058\n",
      "ac [[0.16092198 1.         1.         0.84857666 1.         1.        ]] ua [[ 0.16233306 46.119247   32.52089     1.2510462  55.37851    33.05904   ]]\n",
      "292 -th evaluation done. score: 6351.795746308765 step: 1000\n",
      "step 1000 score 6053.018837477183 qloss 35.716896 ploss -526.20465\n",
      "ac [[ 1. -1.  1. -1. -1.  1.]] ua [[108.86898  -83.124405 120.381226 -55.565617 -90.411476  97.857376]]\n",
      "293 -th evaluation done. score: 6326.613461669159 step: 1000\n",
      "step 1000 score 6244.611294073079 qloss 11.030579 ploss -532.17206\n",
      "ac [[-0.33975038  1.          1.          0.72719854  1.          1.        ]] ua [[-0.35381034 36.546455   41.398743    0.92275584 43.54564    38.162643  ]]\n",
      "294 -th evaluation done. score: 6237.388235204344 step: 1000\n",
      "step 1000 score 6408.243631902871 qloss 17.883001 ploss -526.83307\n",
      "ac [[0.08721126 1.         1.         0.9803655  1.         1.        ]] ua [[ 0.08743338 29.724482   45.765266    2.306875   31.6548     41.90441   ]]\n",
      "295 -th evaluation done. score: 5919.3617663070745 step: 1000\n",
      "step 1000 score 6314.901038898421 qloss 13.485227 ploss -529.49097\n",
      "ac [[0.07005082 1.         1.         0.9904972  1.         1.        ]] ua [[ 0.07016575 34.186863   21.976616    2.672272   35.003246   36.72486   ]]\n",
      "296 -th evaluation done. score: 6242.737660379173 step: 1000\n",
      "step 1000 score 6196.65278552953 qloss 10.48073 ploss -527.37885\n",
      "ac [[-0.17252554  1.          1.          0.54044956  1.          1.        ]] ua [[-0.17426854 32.637688   43.154118    0.60479033 45.516045   28.288551  ]]\n",
      "297 -th evaluation done. score: 6404.782471997076 step: 1000\n",
      "step 1000 score 6361.8562602079555 qloss 15.271531 ploss -529.15424\n",
      "ac [[0.25999665 1.         1.         0.9958538  1.         1.        ]] ua [[ 0.26610482 24.061878   39.410793    3.0883145  47.350304   36.54545   ]]\n",
      "298 -th evaluation done. score: 6145.755877256 step: 1000\n",
      "step 1000 score 6270.033468772428 qloss 24.003464 ploss -529.03046\n",
      "ac [[0.22101055 1.         1.         0.9679862  1.         1.        ]] ua [[ 0.2247183 39.440994  43.081146   2.0593004 38.40228   49.23478  ]]\n",
      "299 -th evaluation done. score: 6516.765829157451 step: 1000\n",
      "step 1000 score 6047.409285856491 qloss 16.285759 ploss -531.8304\n",
      "ac [[-0.5033569   1.          1.          0.38931277  1.          1.        ]] ua [[-0.55379206 31.817364   52.63186     0.41098982 23.46413    35.662342  ]]\n",
      "300 -th evaluation done. score: 6218.82825396896 step: 1000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "agent = Agent()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "agent.q_initial_sync()\n",
    "## TRAIN ##\n",
    "eval_scores = []\n",
    "training_scores = []\n",
    "for episode in range(MAX_EPISODE):\n",
    "    state = env.reset()\n",
    "\n",
    "#     eps = [[np.random.normal(agent.action_dim)]]\n",
    "#     aa = sess.run(agent.action, feed_dict={agent.state_ph:[state], agent.action_eps_ph:eps})\n",
    "#     d=sess.run(agent.log_policy, feed_dict={agent.state_ph:[state], agent.action_ph:aa, agent.action_eps_ph:eps})\n",
    "#     print('d',d)\n",
    "#     print('sat',state)\n",
    "    score = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        action, ua = agent.sample_action(state)\n",
    "        \n",
    "#         if episode%10==0:\n",
    "#             env.render()\n",
    "#             time.sleep(0.01)\n",
    "        next_state, reward, done, info = env.step(action[0])\n",
    "        fake_done = False\n",
    "        time.sleep(.002)\n",
    "#         env.render()\n",
    "#         time.sleep(0.01)\n",
    "        agent.replay.update(state, action[0], [reward], next_state, [fake_done])\n",
    "#         print('nn',next_state)\n",
    "        qloss, ploss = agent.train()\n",
    "\n",
    "\n",
    "        step += 1\n",
    "        score += reward\n",
    "        state = next_state\n",
    "        if done or step>2000:\n",
    "            training_scores.append(score)\n",
    "            print('step', step, 'score', score, 'qloss', qloss,'ploss',ploss)\n",
    "            print('ac', action, 'ua', ua)\n",
    "            print\n",
    "            break\n",
    "    \n",
    "    eval_score = agent.evaluate()\n",
    "    eval_scores.append(eval_score)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodmodel'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess,'goodmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59.555935 , -41.77925  ,  84.87851  ,  68.1167   , -52.659412 ,\n",
       "         -0.8460651]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = agent.sample_action_eps()\n",
    "sess.run(agent.action_mu, feed_dict={agent.state_ph:[state], agent.action_eps_ph:[eps]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "done 6276.299460124448\n"
     ]
    }
   ],
   "source": [
    "state = eval_env.reset()\n",
    "testsc = 0\n",
    "while True:\n",
    "    action = agent.sample_action(state)\n",
    "    next_state, reward, done, info = eval_env.step(action[0])\n",
    "    testsc += reward\n",
    "    eval_env.render()\n",
    "#     time.sleep(0.02)\n",
    "    if done:\n",
    "        print('done', testsc)\n",
    "        eval_env.close()\n",
    "        break\n",
    "    state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8f5b2c2ef0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(eval_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8f5ae199e8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.89449114e-02  8.24127793e-03 -9.08056190e-02 -6.14158210e-03\n",
      "  9.83831413e-02 -4.98359380e-02  8.56840804e-02 -4.62104376e-02\n",
      "  7.98437363e-02 -2.06938379e-01  4.54899636e-02 -8.23456727e-02\n",
      "  1.65275783e-01 -2.42939989e-02  8.69395109e-02  1.34701536e-04\n",
      "  9.08832051e-03]\n"
     ]
    },
    {
     "ename": "MujocoException",
     "evalue": "Unknown warning type Time = 0.0000.Check for NaN in simulation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMujocoException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-6a11f06d64d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#             env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#             time.sleep(0.01)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mfake_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/gym/envs/mujoco/half_cheetah_v3.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx_position_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx_position_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         x_velocity = ((x_position_after - x_position_before)\n",
      "\u001b[0;32m~/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36mdo_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctrl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     def render(self,\n",
      "\u001b[0;32m~/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/mujoco_py/mjsim.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.MjSim.step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/mujoco_py/cymj.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.wrap_mujoco_warning.__exit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/mujoco_py/cymj.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.c_warning_callback\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mujoco_pjh/lib/python3.6/site-packages/mujoco_py/builder.py\u001b[0m in \u001b[0;36muser_warning_raise_exception\u001b[0;34m(warn_bytes)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;31m# This unhelpfully-named warning is what you get if you feed MuJoCo NaNs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'Unknown warning type'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMujocoException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Check for NaN in simulation.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mMujocoException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Got MuJoCo Warning: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMujocoException\u001b[0m: Unknown warning type Time = 0.0000.Check for NaN in simulation."
     ]
    }
   ],
   "source": [
    "## 리스토어 버전\n",
    "\n",
    "# import time\n",
    "# tf.reset_default_graph()\n",
    "env.close()\n",
    "env = gym.make(GAME_NAME)\n",
    "eval_env = gym.make(GAME_NAME)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# agent = Agent()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# agent.q_initial_sync()\n",
    "## TRAIN ##\n",
    "# eval_scores = []\n",
    "# training_scores = []\n",
    "for episode in range(MAX_EPISODE-192):\n",
    "    state = env.reset()\n",
    "\n",
    "#     eps = [[np.random.normal(agent.action_dim)]]\n",
    "#     aa = sess.run(agent.action, feed_dict={agent.state_ph:[state], agent.action_eps_ph:eps})\n",
    "#     d=sess.run(agent.log_policy, feed_dict={agent.state_ph:[state], agent.action_ph:aa, agent.action_eps_ph:eps})\n",
    "#     print('d',d)\n",
    "#     print('sat',state)\n",
    "    score = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        action, ua = agent.sample_action(state)\n",
    "        print()\n",
    "#         if episode%10==0:\n",
    "#             env.render()\n",
    "#             time.sleep(0.01)\n",
    "        next_state, reward, done, info = env.step(action[0])\n",
    "        fake_done = False\n",
    "        time.sleep(.002)\n",
    "#         env.render()\n",
    "#         time.sleep(0.01)\n",
    "        agent.replay.update(state, action[0], [reward], next_state, [fake_done])\n",
    "#         print('nn',next_state)\n",
    "        qloss, ploss = agent.train()\n",
    "\n",
    "\n",
    "        step += 1\n",
    "        score += reward\n",
    "        state = next_state\n",
    "        if done or step>2000:\n",
    "            training_scores.append(score)\n",
    "            print('step', step, 'score', score, 'qloss', qloss,'ploss',ploss)\n",
    "            print('ac', action, 'ua', ua)\n",
    "            print\n",
    "            break\n",
    "    \n",
    "    eval_score = agent.evaluate()\n",
    "    eval_scores.append(eval_score)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_pjh_kernel",
   "language": "python",
   "name": "mujoco_pjh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
